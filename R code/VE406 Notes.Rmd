---
title: "Applied Regression Analysis using R"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---
# L01-Basic 
use '_' in name, to avoid confusion with the member function
```{r}
# help
rm(list = ls()) # clean environment 
?is.logical()
library(help = "stats") # for function list in a package
is.integer(7L) # integer, with 'L' after number
is.integer(7)
x <- 1 # assignment 
1 -> x1
x = 1

# vector c()
month <- c('Jan','Feb','Dec')
class(month)
num <- 1:50 # return a vector 
num_seq <- seq(1, 50, by = 2) # create sequence: seq(from, to, by)
seq(stats::rnorm(20))
num_seq[which(num_seq>9)]

# categorical data
month.fac <- factor(month, order=TRUE, levels = c("Jan", "Feb", "Dec"))
month.fac.in <- factor(month) # default: mathematical order

# matrix, special vectors in R 
A = matrix(month, nrow = 6, byrow = TRUE, ncol = 9)
B = matrix(month, nrow = 6, byrow = FALSE, ncol = 9)

# list, combine different data type 
listAB = list(num = num_seq, Amatrix = A, Bmatrix = B)
class(listAB)

# data.frame
df.A <- data.frame(A)
df.A$X1
class(df.A)
class(df.A$X3)
df.A$X1 <- factor(df.A$X1)

# function
myfuc = function(x) { # key word function
    s = 0
    for (eps in x) {
      if (eps <= 0){
        next
      }
      s = s + eps
      if (s > 20) {
        break
      } 
    }
    s
}

# recycling rule
c(1, 2, 3, 4) + c(1, 2) # equal to c(1, 2, 3, 4) + c(1, 2, 1, 2)

```
e
## implicit coercion to mixed types
logical -> integer -> numeric -> complex -> character

as transform from logical to integer is easier

```c(11, month)```  will convert integer to a character 

# L02-slr
*simple linear regression basic* 

- regression analysis: statistical model that involve one dependent variable and one or more independent variables 

- regression: find a rule of picking distribution for *Y* from a space of infinitely many distribution that agrees with the data

 **Primary Assumption:** (for now) a sequence of random variables is independent and identically distributed (i.i.d.)
 
## Estimation: 

The process of using data to suggest a value for a parameter 

**Estimate:** the value suggested is called the estimate of the parameter (depend on data)

**Estimator:** a function of the data, that gives estimates of the parameter
$\hat{\theta} = h(X)$
, the distribution for $\hat{\theta}$ is called **sampling distribution** of the estimator.
```{r}
?pbinom
```
Given X~Normal(n, p)

Pr(X = x) = dbinom(x, size = n, prob = p)

Pr(X <= x) = pbinom(x, size = n, prob = p)

qbinom: The quantile is defined as the smallest value x such that F(x) ≥ p, where F is the distribution function. with p$\in$ [0,1], returns the smallest value of q s.t. Pr(X<=returnvalue) >= p

```{r}
dbinom(x, size, prob, log = FALSE)
pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
rbinom(n, size, prob)
```


### Binomial Example

*L02-16/55 pages, US presidents*

- if we become US presidents, we are more likely to have sons -- T/F depend on the p-value 
- US presidents are more likely to have sons -- independent of the p-value, because the current data has exhausted all the info of presidents

```{r}
rm(list = ls())  # clean working environment

## set up hypothesis
theta <- 1/2  ## assume equal chance 50-50
n <- 158  ## total number of trials
x <- 0:n  ## all possible number
x1 <- seq(0, n, by = 1)  # alternative way of set up sequence
fX <- dbinom(x, size = n, prob = theta)  ## pdf of x
# ? dbinom  ## getting help with ?
obs <- 67
p.lowerTail <- pbinom(obs, size = n, prob = theta)
## under the assumption of theta = 0.5
p.value <- 2 * p.lowerTail

## visualization
tmpL <- fX[x <= 67]  ## as extreme as what we observed
x.upperTail <- 1 + qbinom(p.lowerTail, 
                          size = n, prob = theta, lower.tail = FALSE) ## quantile function, different from pbinom 
tmpU <- fX[x >= x.upperTail]

M <- x[x > 67 & x < x.upperTail]
tmpM <- fX[x > 67 & x < x.upperTail]

plot(x, fX, type = "n", 
     xlab = "observed number of daughter", ylab = "probability mass")
lines(0:obs, tmpL,type = "h", col = "red")
lines(x.upperTail:n, tmpU,type = "h", col = "red")
lines(M, tmpM, tmpM,type = "h")
points(0:obs, tmpL, col = "red")
points(x.upperTail:n, tmpU, col = "red")
points(M, tmpM)
legend("topright", legend = "P-value", col = "red", lty = 1, pch = 1)
```

```{r}
## function to calculate the density of a binomial distribution
## modified with two more attributes: obs and n
myp_func <- function(n, obs, p){
  ## n: total number of binomial trials
  ## obs: observed number of events
  ## p: probability of success
  pbinom(obs, size = n, prob = p) - pbinom(obs - 1, size = n, prob = p)
  ## exactly the same as dbinom(obs, size = n, prob = p)
}

myp_func(190, 125, 0.5)
dbinom(125, size = 190, prob = 0.5)

myp_func(190, 125, 0.6)/myp_func(190, 125, 0.5)
dbinom(125, size = 190, prob = 0.6)/dbinom(125, size = 190, prob = 0.5)

## to find the maximum likelihood estimate based on the obs
## we need to find the p that maximise the underlying pdf/pmf
## myp_func is here to show how to write a simple function
## we will use the proper dbinom function here after


## to visually examine the mle of deep sea diver case study
## observations: 125 daughter of 190 children
pvec <- seq (0, 1, length.out = 10000000)  ## set the seq of all possible prob
lvec <- dbinom(125, size = 190, prob = pvec)  ## find the likelihood
plot (pvec, lvec, type = "l",  ## visualisation of the likelihood
      xlab = "True probability of having a daughter for a male deep sea diver",
      ylab = "Likelihood of observing 125 daughters of 190 children",
      main = "Likelihood fuction",
      xlim = c(-0.2, 1))

# common sense: best guess: the estimator of p is 125/190
# get from the maximum likelihood function
phat <- pvec[which.max(lvec)]
abline(v = phat, col = "red")
legend("topleft", legend = "Maximum Likelihood Estimate",
       lty = 1, col = 2)
# get that phat = 0.658...

```
### Confidence Interval
Confidence interval of binomal distribution 
- When says 

We are 95\% confident that the true probability of having a daughter is between (0.586, 0.725) for a male deep-sea diver 

it is actually means 

The method used to obtain the interval (0.586, 0.725) will contain the true probability 95\% of time if it is to be repeated 
```{r}
binom.test(125, n = 190, p = 0.5) 
```
```{r}
binom::binom.confint(125, n = 190)
```

```{r}
rm(list = ls())
num = 1e3
n = 190
runif(1)
```

## Law of large numbers 
Suppose that $X_1,X_2,.. X_n$ all have the same expected value $\mathbb{E}[X_i]=\mu$, the same variance $Var[X_i] = \sigma^2$ and zero covariance with each other $Cov[X_i, X_j] = 0$, when $i\neq j$

or could say if $X_i$ are i.i.d., then the following holds 
$$\bar{X_n} = \frac{1}{n}\sum_{i=1}^nX_i \rightarrow \mu  ,when \quad n\rightarrow \infty $$

reduce the spread of the distribution to 0 effectively 
$$\lim_{n\rightarrow \infty} Pr(|\bar{X_n} - \mu| > \varepsilon) = 0, \rm{for \ any} \ \varepsilon > 0 $$

### Illustration of LLN
As n increases, the mean of X approaches the true mean and the error approaches 0.
```{r}
rm(list = ls())
n <- 1e4
lambda <- 3

xpois <- rpois(n, lambda)  ## consider poisson random variable
xexp <- lambda  ## true underlying mean

nseq <- 1:n
# investigate Xbar when n increases, `cumsum` is  cumulative sum 
xcbar <- cumsum(xpois)/nseq  ## sample mean as n increases, at various stage 
error <- abs(xcbar - xexp)  ## relative error
plot(nseq, xcbar, type = "l", xlab = "Sample Size",
     ylab = "Sample Mean")
abline(h = xexp, col = 2)
legend("topright", legend = "True mean", lty = 1, col = 2)


plot(nseq, error, type = "l", xlab = "Sample Size", ylab = "Relative Error")
abline(h = 0, col = 2)
```
A better look:
```{r}
## a better look at LLN
sample.size.vec <- c(5, 10, 50, 100, 500)  ## 5 cases
ncases <- length(sample.size.vec)
num <- 1e3  ## number of repetitions
xbar.vec <- double()  ## x bar for all simulations
error.vec <- double()  ## error for all simulations
n.vec <- integer()  ## sample size for each case

for(j in 1:ncases){
  n <- sample.size.vec[j]  ## current sample size
  s.vec <- double(num)  ## all x bar for this n
  e.vec <- double(num)  ## all error for this n

  for (i in 1:num){  ## repeat num number of times
    x <- rpois(n, lambda)  ## sample of x by sample size 5, 10 etc.
    s.vec[i] <- sum(x)/n
    e.vec[i] <- abs(s.vec[i] - xexp)/abs(xexp)
  }
  
  xbar.vec <- c(xbar.vec, s.vec)
  error.vec <- c(error.vec, e.vec)
  n.vec <- c(n.vec, rep(sample.size.vec[j], num))
  }

x.df <- data.frame(xbar = xbar.vec, error = error.vec, n = n.vec)
boxplot(xbar~n, data = x.df, xlab = "Sample Size", ylab = "Sample Mean")
abline(h = xexp, col = 2)
legend("topright", legend = "True mean",
       lty = 1, col = 2)

boxplot(error~n, data = x.df,
        xlab = "Sample Size", ylab = "Relative Error")
abline(h = 0, col = 2)
## as we increase the sample size
## the error gets closer to 0 and more constant

```



## Central Limit Theorm 

![](/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/image/CLT.png)
### Illustration 
```{r}
sample.size.vec
lambda; xexp

xvar <- lambda  ## true variance for Poisson
sqrt.n.vec <- sqrt(n.vec)  ## last for loop

sample_error <- xbar.vec - xexp
sample_variance <- sqrt(xvar)
r.vec <- sqrt.n.vec*sample_error/sample_variance  ## normalised error

x.hist <- seq(-4, 4, length.out = 100)
par(mfrow = c(2, 3))

for(eps in sample.size.vec){
  ss <- r.vec[n.vec == eps]  ## subset by n
  tname <- bquote(bold(atop("Sample Size = "~.(eps), "1000 Repetition")))
  
  hist(ss, feq = FALSE, xlab = "r", main = tname,
       ylim = c(0, 0.6), xlim = c(-4, 4))

  lines(density(ss), col = 4, lty = 2)  ## kernel denity estimation
  lines(x.hist, dnorm(x.hist), col = 2, lty = 1)  ## true density function
  legend("top", col = c(1, 2), lty = c(2, 1),
         legend = c("Estimation", "True"))
}

true.norm <- rnorm(num, mean = 0, sd = 1)
hist(true.norm, freq = FALSE, ylim = c(0, 0.6),
     xlim = c(-4, 4), xlab = "Z", main = "100 Z~N(0, 1)")
lines(x.hist, dnorm(x.hist), col = 2, lty = 1)
par(mfrow = c(1, 1))

sample_quantile_5 = sort(r.vec[n.vec == 5])
sample_quantile_10 = sort(r.vec[n.vec == 10])
sample_quantile_50 = sort(r.vec[n.vec == 50])
sample_quantile_100 = sort(r.vec[n.vec == 100])
sample_quantile_500 = sort(r.vec[n.vec == 500])
sample_cdf = (1:num)/num
plot(sample_quantile_5, sample_cdf, type = "s",
     xlab = "", ylab = "",
     main = "Cumulative distribution function n = 500")
lines(sample_quantile_10, sample_cdf, col = 3, lty = 4)
lines(sample_quantile_50, sample_cdf, col = 4, lty = 4)
lines(sample_quantile_100, sample_cdf, col = 5, lty = 4)
lines(sample_quantile_500, sample_cdf, col = 6, lty = 4)
lines(x.hist, pnorm(x.hist), col = 2, lty = 2)
legend("topleft", lty = c(1, 2), col = c(1, 2), 
       legend = c("Estimation", "True CDF"))


```


# L03 - LSE & MLE

$MSE(m) = \mathbb{E}[(Y-m)^2] = Var[Y] + (\mathbb{E}[Y-m])^2$ -> bias variance decomposition

The common model assumptions for slr 

bias of an estimator $\hat{\theta}$, $\theta $ is a fixed population parameter 
$Bias(\hat{\theta},\theta) = \mathbb{E}[\hat{\theta} - \theta]$

- residual 

the estimate $\hat{e_i}$ is known as **residual** .

observed $y_i$ and $x_i$,
$\hat{e_i} =y_i - ( b_0 + b_1x_i)$, where $b_0$ and $b_1$ are estimates of $\beta_0$ and $\beta_1$; $\hat{y_i} = b_0 + b_1x_i$ is known as the predicted/fitted value. 

Then  $y_i = \hat{y_i}+\hat{e_i}$

- consistent 

An estimator is **consistent** if
$$\hat{\theta_n} \rightarrow \theta \quad \rm{when} \quad n\rightarrow \infty$$
### Least Square Estimation 

$SS_E$: error sum of squares 

$SS_E:=e_1^2 + e_2^2 + ...+e_n^2 = \sum_{i=1}^n(y_i-(b_0+b_1x_i))^2$

## model assumption for slr
![](/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/image/modelAss.png)

## assumptions about the error term 



# Overview of statistics 
In true universe, we have random variable $Y$;

In real world, we observe value $y$;

Then to link them together, we use **estimator**. However, error exist in this process, two types 

- sampling error (improve when data collection)
- estimation error (improve using statistics）

Two main things 
- parameter estimation 
- hypothesis testing 

To test the model sufficiency 

- F-Test for significance of regression: whether all predictors are zero 
- T-test: whether a single predictor is necessary for a given model
- Partial F-test: can be applied to an arbitrary subset of predictors 




# L04 - diagnostics



## residual plot 

$\hat{e_i}\quad vs. \quad \hat{y_i}$ fitted value of y

the variance of $\varepsilon$ in true world is expected to be a constant, the variance of the residuals could be shown it is also a constant. 


To check the assumptions of the residual

- 1. In residual plot, it should be random scatter, no underlying non-linear relationship
- 2. In residual plot, randomly scatter around mean zero, with basically unchanged variance 
- 3. check they are independent of each other by ACF 
- 4. check normality by QQplot (Quantile-Quartile normal plot)

## autocorrelation function
If errors are independent, the estimates should be small and patternless 

time series variable - very likely to be underlying correlation among residuals 

do the residual plot after fit the tesla price model

observe some high price - after days are likely to be large

based on ACF plot: determine the underlying lag correlation 

bad impact - negative on the following days 

tesla price today ~ tomorrow? ~ next week? :**autocorrelation function**

hypothesis -> colloect data -> describe the data use (sample mean/.../plot) -> fit the model -> use diagnostics to check the assumption (through residual ~ plot residual vs. fitted value / residuali vs. residualj 或者 autocorralation check coevariance / residuals vs. X) --- if the assumption is violated -> do the transformation ( constant variance  ~ log trandformation of y / normality -> box-cox to find additional variable, 可以同时对于y与x transform) 

# L05 - transformation

## Box-Cox transformation 

When y and x both do not follow normal dist., use transformation 
find the $\lambda$ that maximize the likelihood function




