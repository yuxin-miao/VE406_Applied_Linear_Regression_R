---
title: "Applied Regression Analysis using R"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---
# L01-Basic 
use '_' in name, to avoid confusion with the member function
```{r}
# help
rm(list = ls()) # clean environment 
?is.logical()
library(help = "stats") # for function list in a package
is.integer(7L) # integer, with 'L' after number
is.integer(7)
x <- 1 # assignment 
1 -> x1
x = 1

# vector c()
month <- c('Jan','Feb','Dec')
class(month)
num <- 1:50 # return a vector 
num_seq <- seq(1, 50, by = 2) # create sequence: seq(from, to, by)
seq(stats::rnorm(20))
num_seq[which(num_seq>9)]

# categorical data
month.fac <- factor(month, order=TRUE, levels = c("Jan", "Feb", "Dec"))
month.fac.in <- factor(month) # default: mathematical order

# matrix, special vectors in R 
A = matrix(month, nrow = 6, byrow = TRUE, ncol = 9)
B = matrix(month, nrow = 6, byrow = FALSE, ncol = 9)

# list, combine different data type 
listAB = list(num = num_seq, Amatrix = A, Bmatrix = B)
class(listAB)

# data.frame
df.A <- data.frame(A)
df.A$X1
class(df.A)
class(df.A$X3)
df.A$X1 <- factor(df.A$X1)

# function
myfuc = function(x) { # key word function
    s = 0
    for (eps in x) {
      if (eps <= 0){
        next
      }
      s = s + eps
      if (s > 20) {
        break
      } 
    }
    s
}

# recycling rule
c(1, 2, 3, 4) + c(1, 2) # equal to c(1, 2, 3, 4) + c(1, 2, 1, 2)

```
e
## implicit coercion to mixed types
logical -> integer -> numeric -> complex -> character

as transform from logical to integer is easier

```c(11, month)```  will convert integer to a character 

# L02-slr
*simple linear regression basic* 

- regression analysis: statistical model that involve one dependent variable and one or more independent variables 

- regression: find a rule of picking distribution for *Y* from a space of infinitely many distribution that agrees with the data

 **Primary Assumption:** (for now) a sequence of random variables is independent and identically distributed (i.i.d.)
 
## Estimation: 

The process of using data to suggest a value for a parameter 

**Estimate:** the value suggested is called the estimate of the parameter (depend on data)

**Estimator:** a function of the data, that gives estimates of the parameter
$\hat{\theta} = h(X)$
, the distribution for $\hat{\theta}$ is called **sampling distribution** of the estimator.
```{r}
?pbinom
```
Given X~Normal(n, p)

Pr(X = x) = dbinom(x, size = n, prob = p)

Pr(X <= x) = pbinom(x, size = n, prob = p)

qbinom: The quantile is defined as the smallest value x such that F(x) ≥ p, where F is the distribution function. with p$\in$ [0,1], returns the smallest value of q s.t. Pr(X<=returnvalue) >= p

```{r}
dbinom(x, size, prob, log = FALSE)
pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
rbinom(n, size, prob)
```


### Binomial Example

*L02-16/55 pages, US presidents*

- if we become US presidents, we are more likely to have sons -- T/F depend on the p-value 
- US presidents are more likely to have sons -- independent of the p-value, because the current data has exhausted all the info of presidents

```{r}
rm(list = ls())  # clean working environment

## set up hypothesis
theta <- 1/2  ## assume equal chance 50-50
n <- 158  ## total number of trials
x <- 0:n  ## all possible number
x1 <- seq(0, n, by = 1)  # alternative way of set up sequence
fX <- dbinom(x, size = n, prob = theta)  ## pdf of x
# ? dbinom  ## getting help with ?
obs <- 67
p.lowerTail <- pbinom(obs, size = n, prob = theta)
## under the assumption of theta = 0.5
p.value <- 2 * p.lowerTail

## visualization
tmpL <- fX[x <= 67]  ## as extreme as what we observed
x.upperTail <- 1 + qbinom(p.lowerTail, 
                          size = n, prob = theta, lower.tail = FALSE) ## quantile function, different from pbinom 
tmpU <- fX[x >= x.upperTail]

M <- x[x > 67 & x < x.upperTail]
tmpM <- fX[x > 67 & x < x.upperTail]

plot(x, fX, type = "n", 
     xlab = "observed number of daughter", ylab = "probability mass")
lines(0:obs, tmpL,type = "h", col = "red")
lines(x.upperTail:n, tmpU,type = "h", col = "red")
lines(M, tmpM, tmpM,type = "h")
points(0:obs, tmpL, col = "red")
points(x.upperTail:n, tmpU, col = "red")
points(M, tmpM)
legend("topright", legend = "P-value", col = "red", lty = 1, pch = 1)
```

```{r}
## function to calculate the density of a binomial distribution
## modified with two more attributes: obs and n
myp_func <- function(n, obs, p){
  ## n: total number of binomial trials
  ## obs: observed number of events
  ## p: probability of success
  pbinom(obs, size = n, prob = p) - pbinom(obs - 1, size = n, prob = p)
  ## exactly the same as dbinom(obs, size = n, prob = p)
}

myp_func(190, 125, 0.5)
dbinom(125, size = 190, prob = 0.5)

myp_func(190, 125, 0.6)/myp_func(190, 125, 0.5)
dbinom(125, size = 190, prob = 0.6)/dbinom(125, size = 190, prob = 0.5)

## to find the maximum likelihood estimate based on the obs
## we need to find the p that maximise the underlying pdf/pmf
## myp_func is here to show how to write a simple function
## we will use the proper dbinom function here after


## to visually examine the mle of deep sea diver case study
## observations: 125 daughter of 190 children
pvec <- seq (0, 1, length.out = 10000000)  ## set the seq of all possible prob
lvec <- dbinom(125, size = 190, prob = pvec)  ## find the likelihood
plot (pvec, lvec, type = "l",  ## visualisation of the likelihood
      xlab = "True probability of having a daughter for a male deep sea diver",
      ylab = "Likelihood of observing 125 daughters of 190 children",
      main = "Likelihood fuction",
      xlim = c(-0.2, 1))

# common sense: best guess: the estimator of p is 125/190
# get from the maximum likelihood function
phat <- pvec[which.max(lvec)]
abline(v = phat, col = "red")
legend("topleft", legend = "Maximum Likelihood Estimate",
       lty = 1, col = 2)
# get that phat = 0.658...

```
### Confidence Interval
Confidence interval of binomal distribution 
- When says 

We are 95\% confident that the true probability of having a daughter is between (0.586, 0.725) for a male deep-sea diver 

it is actually means 

The method used to obtain the interval (0.586, 0.725) will contain the true probability 95\% of time if it is to be repeated 
```{r}
binom.test(125, n = 190, p = 0.5) 
```
```{r}
binom::binom.confint(125, n = 190)
```

```{r}
rm(list = ls())
num = 1e3
n = 190
runif(1)
```

## Law of large numbers 
Suppose that $X_1,X_2,.. X_n$ all have the same expected value $\mathbb{E}[X_i]=\mu$, the same variance $Var[X_i] = \sigma^2$ and zero covariance with each other $Cov[X_i, X_j] = 0$, when $i\neq j$

or could say if $X_i$ are i.i.d., then the following holds 
$$\bar{X_n} = \frac{1}{n}\sum_{i=1}^nX_i \rightarrow \mu  ,when \quad n\rightarrow \infty $$

reduce the spread of the distribution to 0 effectively 
$$\lim_{n\rightarrow \infty} Pr(|\bar{X_n} - \mu| > \varepsilon) = 0, \rm{for \ any} \ \varepsilon > 0 $$

### Illustration of LLN
As n increases, the mean of X approaches the true mean and the error approaches 0.
```{r}
rm(list = ls())
n <- 1e4
lambda <- 3

xpois <- rpois(n, lambda)  ## consider poisson random variable
xexp <- lambda  ## true underlying mean

nseq <- 1:n
# investigate Xbar when n increases, `cumsum` is  cumulative sum 
xcbar <- cumsum(xpois)/nseq  ## sample mean as n increases, at various stage 
error <- abs(xcbar - xexp)  ## relative error
plot(nseq, xcbar, type = "l", xlab = "Sample Size",
     ylab = "Sample Mean")
abline(h = xexp, col = 2)
legend("topright", legend = "True mean", lty = 1, col = 2)


plot(nseq, error, type = "l", xlab = "Sample Size", ylab = "Relative Error")
abline(h = 0, col = 2)
```
A better look:
```{r}
## a better look at LLN
sample.size.vec <- c(5, 10, 50, 100, 500)  ## 5 cases
ncases <- length(sample.size.vec)
num <- 1e3  ## number of repetitions
xbar.vec <- double()  ## x bar for all simulations
error.vec <- double()  ## error for all simulations
n.vec <- integer()  ## sample size for each case

for(j in 1:ncases){
  n <- sample.size.vec[j]  ## current sample size
  s.vec <- double(num)  ## all x bar for this n
  e.vec <- double(num)  ## all error for this n

  for (i in 1:num){  ## repeat num number of times
    x <- rpois(n, lambda)  ## sample of x by sample size 5, 10 etc.
    s.vec[i] <- sum(x)/n
    e.vec[i] <- abs(s.vec[i] - xexp)/abs(xexp)
  }
  
  xbar.vec <- c(xbar.vec, s.vec)
  error.vec <- c(error.vec, e.vec)
  n.vec <- c(n.vec, rep(sample.size.vec[j], num))
  }

x.df <- data.frame(xbar = xbar.vec, error = error.vec, n = n.vec)
boxplot(xbar~n, data = x.df, xlab = "Sample Size", ylab = "Sample Mean")
abline(h = xexp, col = 2)
legend("topright", legend = "True mean",
       lty = 1, col = 2)

boxplot(error~n, data = x.df,
        xlab = "Sample Size", ylab = "Relative Error")
abline(h = 0, col = 2)
## as we increase the sample size
## the error gets closer to 0 and more constant

```



## Central Limit Theorm 

![](/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/image/CLT.png)
### Illustration 
```{r}
sample.size.vec
lambda; xexp

xvar <- lambda  ## true variance for Poisson
sqrt.n.vec <- sqrt(n.vec)  ## last for loop

sample_error <- xbar.vec - xexp
sample_variance <- sqrt(xvar)
r.vec <- sqrt.n.vec*sample_error/sample_variance  ## normalised error

x.hist <- seq(-4, 4, length.out = 100)
par(mfrow = c(2, 3))

for(eps in sample.size.vec){
  ss <- r.vec[n.vec == eps]  ## subset by n
  tname <- bquote(bold(atop("Sample Size = "~.(eps), "1000 Repetition")))
  
  hist(ss, feq = FALSE, xlab = "r", main = tname,
       ylim = c(0, 0.6), xlim = c(-4, 4))

  lines(density(ss), col = 4, lty = 2)  ## kernel denity estimation
  lines(x.hist, dnorm(x.hist), col = 2, lty = 1)  ## true density function
  legend("top", col = c(1, 2), lty = c(2, 1),
         legend = c("Estimation", "True"))
}

true.norm <- rnorm(num, mean = 0, sd = 1)
hist(true.norm, freq = FALSE, ylim = c(0, 0.6),
     xlim = c(-4, 4), xlab = "Z", main = "100 Z~N(0, 1)")
lines(x.hist, dnorm(x.hist), col = 2, lty = 1)
par(mfrow = c(1, 1))

sample_quantile_5 = sort(r.vec[n.vec == 5])
sample_quantile_10 = sort(r.vec[n.vec == 10])
sample_quantile_50 = sort(r.vec[n.vec == 50])
sample_quantile_100 = sort(r.vec[n.vec == 100])
sample_quantile_500 = sort(r.vec[n.vec == 500])
sample_cdf = (1:num)/num
plot(sample_quantile_5, sample_cdf, type = "s",
     xlab = "", ylab = "",
     main = "Cumulative distribution function n = 500")
lines(sample_quantile_10, sample_cdf, col = 3, lty = 4)
lines(sample_quantile_50, sample_cdf, col = 4, lty = 4)
lines(sample_quantile_100, sample_cdf, col = 5, lty = 4)
lines(sample_quantile_500, sample_cdf, col = 6, lty = 4)
lines(x.hist, pnorm(x.hist), col = 2, lty = 2)
legend("topleft", lty = c(1, 2), col = c(1, 2), 
       legend = c("Estimation", "True CDF"))

```


# L03 - LSE & MLE

$MSE(m) = \mathbb{E}[(Y-m)^2] = Var[Y] + (\mathbb{E}[Y-m])^2$ 

-> bias variance decomposition

The common model assumptions for slr 

bias of an estimator $\hat{\theta}$, $\theta$ is a fixed population parameter 
$Bias(\hat{\theta},\theta) = \mathbb{E}[\hat{\theta} - \theta]$

- residual 

the estimate $\hat{e_i}$ is known as **residual** .

observed $y_i$ and $x_i$,
$\hat{e_i} =y_i - ( b_0 + b_1x_i)$, where $b_0$ and $b_1$ are estimates of $\beta_0$ and $\beta_1$; $\hat{y_i} = b_0 + b_1x_i$ is known as the predicted/fitted value. 

Then  $y_i = \hat{y_i}+\hat{e_i}$

- consistent 

An estimator is **consistent** if
$$\hat{\theta_n} \rightarrow \theta \quad \rm{when} \quad n\rightarrow \infty$$
### Least Square Estimation 

$SS_E$: error sum of squares 

$SS_E:=e_1^2 + e_2^2 + ...+e_n^2 = \sum_{i=1}^n(y_i-(b_0+b_1x_i))^2$

## model assumption for slr
![](/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/image/modelAss.png)

## assumptions about the error term 



# Overview of statistics 
In true universe, we have random variable $Y$;

In real world, we observe value $y$;

Then to link them together, we use **estimator**. However, error exist in this process, two types 

- sampling error (improve when data collection)
- estimation error (improve using statistics）

Two main things 
- parameter estimation 
- hypothesis testing 

To test the model sufficiency 

- F-Test for significance of regression: whether all predictors are zero 
- T-test: whether a single predictor is necessary for a given model
- Partial F-test: can be applied to an arbitrary subset of predictors 




# L04 - diagnostics



## residual plot 

$\hat{e_i}\quad vs. \quad \hat{y_i}$ fitted value of y

the variance of $\varepsilon$ in true world is expected to be a constant, the variance of the residuals could be shown it is also a constant. 


To check the assumptions of the residual

- 1. In residual plot, it should be random scatter, no underlying non-linear relationship
- 2. In residual plot, randomly scatter around mean zero, with basically unchanged variance 
- 3. check they are independent of each other by ACF 
- 4. check normality by QQplot (Quantile-Quartile normal plot)




## autocorrelation function
If errors are independent, the estimates should be small and patternless 

time series variable - very likely to be underlying correlation among residuals 

do the residual plot after fit the tesla price model

observe some high price - after days are likely to be large

based on ACF plot: determine the underlying lag correlation 

bad impact - negative on the following days 

tesla price today ~ tomorrow? ~ next week? :**autocorrelation function**

hypothesis -> colloect data -> describe the data use (sample mean/.../plot) -> fit the model -> use diagnostics to check the assumption (through residual ~ plot residual vs. fitted value / residuali vs. residualj 或者 autocorralation check coevariance / residuals vs. X) --- if the assumption is violated -> do the transformation ( constant variance  ~ log trandformation of y / normality -> box-cox to find additional variable, 可以同时对于y与x transform) 



#### Check normality 
plotting the estimated Vs assumed distribution

The error should follow normal distribution N(0, $\sigma^2$)

```{r}
oc.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/old_car.txt", header = TRUE)
str(oc.df)
age = 91 - oc.df$year
oc.LM = lm(price~age, data = oc.df)
model = oc.LM

## Residual vs. fitted values 
plot(fitted.values(model), residuals(model), xlab = "Fitted values", ylab = "Residuals", main = "Residual Plot", sub = "lm(price~age)")
```


# L05 - transformation

$Y_i|X_i$ ~ Normal $(\beta_0+
\beta_1x_i, \sigma^2)$

## Box-Cox transformation 
![](/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/image/boxcox.png)


When y and x both do not follow normal dist., use transformation 
find the $\lambda$ that maximize the likelihood function

如果log transformation是可以的，则会得到lambda的范围基本在0

而都不可以的情况，可以用提供的box-cox，可以同时转化x和y
```{r}
library(MASS)
?boxcox
```


```{r}
oc.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/old_car.txt")
oc.LM = lm(price~age, data = oc.df)
```
# L06 Prediction 
to predict based on the current data 


## TSS
variability in the response variable 

$TSS = \sum^n_{i=1}(y_i-\bar{y})^2=\sum^n_{i=1}(\hat{y_i}-\bar{y})^2 + \sum^n_{i=1} \hat{e_i}^2$
total sum of squares = explained sum of squares + residual sum of squares 

TSS = ESS + RSS 

# L07 Multiple linear regression


![](/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/image/multi.png)

 regression coefficients in multiple regression: partial regression coefficients, as their interpretation requires other variables should be held fixed. 

**How least square works**

$$
\begin{align*}
\textbf{y}  &= \mathbf{X}\hat{\beta} + \hat{e}\\
\rm{minimize\ residual\ sum\ of\ squrares} &\quad\hat{e}^T\hat{e}\\
\rm{obtain\ the\ estimator}&\quad \hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\\
\rm{fitted\ value}&\quad \hat{\textbf{y}}  = \mathbf{X}\hat{\beta}= \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}=\mathbf{P}\mathbf{y} \\
\rm{residual}& \quad \hat{e}=\mathbf{y} -\hat{\mathbf{y}}=(\mathbf{I}-\mathbf{P})\mathbf{y}
\end{align*}
$$
Projection Matrix$\mathbf{P}, \mathbf{P}^T=\mathbf{P}, \mathbf{P}^2=\mathbf{P}$ 

Identity Matrix$I$

unbiased $\mathbf{\hat{\beta}}, \mathbf{\hat{\beta}}\sim\rm{Normal}(\beta,\sigma^2(\mathbf{X^T \mathbf{X}})^{-1})$

Case study: remember the small data set, we do not have strong evidence \
three variables: $y$ as gain; $X$ as driven-in time and dose 

1. drive-in time or dose influences gain linearly O -> F-test \
2. drive-in time influences gain for a given dose linearly -> t-test \
3. dose influences gain for a given drive-in time linearly -> t-test \
4. there is any reason to use both drive-in time and dose to explain  gain -> compare with the submodel using adjusted R-squared 

```{r}
# install.packages("xlsx")
library(xlsx)
trans.df = read.xlsx("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/transistor.xlsx", sheetIndex = 1)
## full model first, take all the regression variables first.
trans.LM = lm(gain~., data = trans.df) # use gain as response, '.' means all other variables are x

plot(trans.LM, which = 1) ## ?
model = trans.LM
fvs = fitted.values(model)
res = residuals(model)
sres = rstandard(model) # standardised 
## before make inference, need to check the assumptions first
# -- residual plot --#
plot(fvs, sres, xlab = "Fitted Values", ylab = "Standardized Residuals", main = "Residual Plot", sub = "lm(formula=gain~.,data = trans.df)") + abline(h = 0, lty = 2, col = "red") # use standardized residuals 
# no indication of nonlinearity, correlation, non-constant variance 

plot(res[-nrow(trans.df)], res[-1], xlab = "Previous", ylab = "Residuals", main = "Residuals vs. Previous Residual", sub = "lm(formula=gain~.,data = trans.df)")
acf(res)
# From both plots, no indication that the residuals are correlated 

qqnorm(res)
qqline(res, col = 2, lty = 2) # right in the R script 

# shapiro-wilk
shapiro.test(res) # small p-value: reject 

## no obvious violation of the assumptions, so we could proceed
summary(model)
```
For question 4, using adjusted R squared which takes the model complexity into account. However, it is only meaningful when **all the model assumptions are met**. 
```{r}
#full model
trans.LM = lm(gain~., data = trans.df)
# submodel 
trans.dose.LM = lm(gain~dose, data = trans.df)
trans.dit.LM = lm(gain~dit, data = trans.df)
# should verify through residual plots then see the summary and compare the adjusted R square
res.dose = residuals(trans.dose.LM)
plot(res.dose)
acf(res.dose)
qqnorm(res.dose) # violate the assumption
shapiro.test(res.dose)

# why
res.dit = residuals(trans.dit.LM)
plot(res.dit)
acf(res.dit)
qqnorm(res.dit) 
shapiro.test(res.dit)

# all violate the assumption, because of the samll dataset, need to be more strict
# summary(trans.dit.LM) summary is meaningless 
```


# L08 - poly
Example1: only drop variables after check residual plots (all the assumptions are satisfied)

```{r}
sim.df = read.csv("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/sim_poly_class.csv.bz2")
str(sim.df)
sim.LM = lm(y~x1+x2+x3, data = sim.df)
plot(sim.LM, which=1)
# plot the residuals, find non-random, indicate nonlinearity
# The nonlinearity would be clearer if add estimated conditional mean to the residual plot 
c.mean.plot = function(tmp.x, tmp.y, n.each, pos = "topleft"){ n.x = rounod(length(unique(tmp.x)) / n.each)
x.group = cut(tmp.x, breaks = n.x, labels = FALSE)
x.vec = tapply(tmp.x, x.group, mean) y.vec = tapply(tmp.y, x.group, mean)
points(x.vec, y.vec, col = "blue", pch = 16) lines(x.vec, y.vec, col = "blue", lty = 2)
legend(pos, "Estimated Conditional Mean", col = "blue", lty = 2, pch = 16)
}

# then do the plot: standardised residual against one of the regressors
# based on the plot, have a new model sim.quad.LM = lm(y~x1+x2+x3+I(x2^2),data = sim.df), then check the assumptions 
# when all assumptions meet, look at the summary for each test 
```


str(LifeCycleSavings)
```{r}
density(~sr, data = LifeCycleSavings)

## Forward, begin with an empty model 



## use anova to do the F-test


## because of the negative correlation between pop15 and pop75, and the small variance of pop75 itself, add one pop75 do not add much information 

## dpi's variance close to infinity (random actually), so still not add much information


## 2和5比较   without pValue -> 2,5 is submodel of full model, 所以2和4可以比较 5和4可以比较 but we could not compare 2 and 5 using F-test (no nested relationship between them)

## model 4 residual plots 
## residual vs. dpi -> very few in 3000-4000 (outliers / points with high leverage / influential points (will influence the regression much   ))


## cook's distance : look at the specific points and determine based on the target whether need to exclude 


## box-cox plot -> reduce the high leverage points (reduce variance in x)

## fit5 
```
need normally distributed y but do not need normally distributed x, because error term relates to y 
two mode / clear gap in the residual plot -> two distributions 

当ACF的蓝色线很宽的时候，说明没有太多的data

# L10 

- stability problem 
```{r}
set.seed(1)
x.vec = rnorm(10, 0, 1)
z.vec = 10*x.vec
tmp = seq(-3, 3, length.out = 10)
y.vec=rnorm(10, mean=x.vec, sd=3)
plot(tmp, 2*tmp, type = "n")
lm(y.vec~x.vec-1) # make the intercept not considered 
replicate(20, {y.vec=rnorm(10, mean=x.vec, sd=3)     abline(lm(y.vec~x.vec-1))})
```

# L 11 
Case Study
shows how to address multicollinearity and unusual points (data cleaning) 
```{r}
chem_pro.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/chem_pro.csv", sep = ",", header = TRUE)
# check 
str(chem_pro.df) # different data type, might be a typo for ratio's type

# convert it to double 
chem_pro.df$ratio = as.character(chem_pro.df$ratio)
chem_pro.df$ratio = as.double(chem_pro.df$ratio)
chem_pro.df$ratio

summary(chem_pro.df) # more test 
# boxcox plot useful to identify unusual values

par(mfrow = c(1,4))
lapply(chem_pro.df, boxplot)
par(mfrow = c(1,1))
# data cleaning result: unusual point in variable 2 and one in variable 4
# So we modify the 44th observation and keep record of it
conversion_typo = which( chem_pro.df$conversion <= -10)
chem_pro.df$conversion[conversion_typo] = - chem_pro.df$conversion[conversion_typo] # then run boxcox again will find usual plot 

# pairs plot 
## put histograms on the diagonal, don't know how to show the number 
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
pairs(chem_pro.df, panel = panel.smooth,
      cex = 1.5, pch = 24, bg = "light blue", horOdd=TRUE,
      diag.panel = panel.hist, cex.labels = 2, font.labels = 2)


model = lm(yield~conversion + flow + ratio, data = chem_pro.df) # howecer we truely want to check 1/ratio
# then check the residual plot vs. fitted value / each regressor (conversion/ratio/flow)
# check independece of residuals, ACF/ previous residual vs. 
#  normqq / small data size, do shapiro.test check normality 

# stabillity problem 
# VIF (stability problem)
Xminus1 = chem_pro.df[, -1]
VIF = diag(solve(cor(Xminus1))) # inverse of diagonal 
VIF #  no larger than 10, no multicollinearity 

# unusual points (influence stability problem)
  # 1. method: pii 
pii.vec = hatvalues(model) # pii: leverage score 
order(pii.vec, decreasing = TRUE)
  # 2. method: influence measure 
im = influence.measures(model)
im # Report and possibly delete influential points
# high leverage, not mean oulier. near regression line: not outlier 

#### check the first theory:Chemical theory predicts yield is related to the reciprocal of ratio
recip.LM = lm(yield~ conversion + flow +
+ I(1/ratio), data = chem_pro.df)
summary(recip.LM) # then have siginificace 1/ratio 
sres = residuals(recip.LM)
recip_outlier_index = which( sres < -2.5 )
# then check the residual plot vs. fitted value / each regressor (conversion/ratio/flow) -> find residual vs. flow need to be trandformed
# -> residual vs. ratiro: curvature due to high leverage point 
recip_hl_index = which(1/chem_pro.df$ratio > 25)
recip_hl_index
ratio_unusual
chem_pro.df[ratio_unusual,]
# check independece of residuals, ACF/ previous residual vs. 
#  normqq / small data size, do shapiro.test check normality 


#### check the second theory: predicts yield is related to conversion*flow (interaction)

prod.LM =lm(yield~ conversion + poly(flow,3) + I(1/ratio) + I(flow*conversion),
data = chem_pro.df)
prod.res = residuals(prod.LM) # check residuals for the assumptions, seems be satisfied 
summary(prod.LM) # do not support interaction 

# check VIF for those 
conversion = chem_pro.df$conversion
flow = poly(chem_pro.df$flow,3)
recip_ratio = 1/chem_pro.df$ratio
prod = chem_pro.df$flow * chem_pro.df$conversion
Xtd = cbind(conversion, flow, recip_ratio, prod)
VIF = diag(solve(cor(Xtd)))
VIF # more than 10  # do not support interaction 


# move back to the cubic model 
cubic.LM = lm(yield~ conversion + poly(flow,3) + + I(1/ratio), data = chem_pro.df)
# and investigate the e↵ect of unusual points on the model cubic.LM.

```

# L12
Heteroskedasticity
```{r}
cleaning.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/cleaning.txt", header = TRUE)
summary(cleaning.df)
cleaning.LM = lm(Rooms~Crews, data = cleaning.df)
crews.group = factor(cleaning.df$Crews)
tapply(cleaning.df$Rooms, crews.group, length) # repeated observations 
convar = tapply(cleaning.df$Rooms,crews.group,var)
convar # conditional sample variance of the response 
v.vec = convar[crews.group] # vector of corresponding conditional variance
w.vec = 1/v.vec # vector of diagonal elements of w
cleaning.WLS = lm(Rooms~Crews, weights = w.vec, data=cleaning.df) # give the desired estimate of \hat{\beta_w}

# then the residual analysis should be done with  \hat{e_w}=W^{1/2}\hat{e}
model = cleaning.WLS
res = residuals(model)
resw = sqrt(w.vec) * res
```

as the $\hat{e_i}$ might be very small/large, work with 

$z_i=ln(\hat{e_i}^2)=2ln|\hat{e_i}|$, the log-scale provides extra numerical stability 


```{r}
cvrv.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/cvrv.csv", sep = ",", header = TRUE)
cvrv.LM = lm(capital~rental, data = cvrv.df)
cvrv.res = residuals(cvrv.LM)
plot(fitted.values(cvrv.LM), cvrv.res) # the variance is clearly unequal, and rental varies continuously., not in the same scale

z = 2 * log(abs(cvrv.LM$residuals)) #  z is the auxiliary response,
auxiliary.LM = lm(z~rental, data = cvrv.df)  # Perform the auxiliary regression
v.vec = exp(auxiliary.LM$fitted.values) # transform back to have the estimated \sigma 
w.vec = 1/v.vec
cvrv.WLS = lm(capital~rental, weights = w.vec, data = cvrv.df) # weight according to the estimated \sigma 
```

# L13
correlated errors, check of independence 
```{r}
sales.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/sales.txt",  sep = "", header = TRUE)
sales.LM = lm(sales~advertising, data = sales.df)
```
 add lag do not help 
 
 fit the residuals (among previous and current residuals)
 
 $\varepsilon_i = p\varepsilon_{i-1} + v_i$, the true error will add the new $v_i$. So mainly useful for prediction. Bias and variance should be reconsidered. 
 
 $v_i$ is normal, so  $\varepsilon_i$  should be normal 
 
 structure : first-order autoregressive process / AR(1) (important in time series)
 
 **improve the model by fitting the residuals with time series issue**
 
 
# L14 variable selection


```{r}
evap.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/evap.txt", header = TRUE)
# install.packages("leaps")

# try to exhaust all the combinations, use RSS to determine. only look at which term is significant or not: for explain but not prediction 
regsubsets.out = leaps::regsubsets( evap~., data = evap.df, nbest=1, nvmax = NULL,method = "exhaustive")  # nbset=1: 1 best model for each p
# NULL for no limit on p


# before using any of the method to pick up the best model, we should first check the model assumption
# when made transformation on regressors, t-test or partial F test should not be used 
# transform the response : redo the variable selection ! 



summary(regsubsets.out) # star means regressor that needed 



plot(regsubsets.out) # related to each of the independent variables 
plot(regsubsets.out, scale = "r2", main = "R^2") # the darker, the better 
plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2")
plot(regsubsets.out, scale = "Cp", main = "Mallow’s Cp")
plot(regsubsets.out, main = "BIC")
```

The validity of the model is a priority especially when the goal is **to explain**.


**to predict, different modern approach**: data splitting / cross-validation / bootstrapping 


to regulazition: 

**ridge regression:** shrinks the number of coefficients by imposing a penalty, encourages the coefficients to be zero. higher $\lambda$, more penalization. drop the inefficient coefficient. L2norm regulazition 
*do the normal transformations to it*

**lasso regression:** 


# L15 nonlinear regression 

Least square estimation for solving non linear regression is same as for linear regression, but no closed-form exists. Numerically solve, start with a good initial value. 

MLE and LSE still identical. in order to solve MLE, has the score equation. 

**Example**
```{r}
plot.new
quartz()
puromycin.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/puromycin.txt", header = TRUE)
str(puromycin.df)

# know the expectation: E[Velocity|Concentration] = (\beta_1 Concentration) / (\beta_2 + Concentration), with unknown parameter \beta_1 and \beta_2

#Create a grid 
p1n = 100; p2n = 100
par1.vec = seq(170, 260, length = p1n) 
par2.vec = seq(0.01, 0.15, length = p2n)
grid = expand.grid(par1.vec, par2.vec)

# RSS function
my.func = function(x){
  fvs = x[1]*puromycin.df[,1]/
    (x[2]+puromycin.df[,1]) 
  res = puromycin.df[,2] - fvs 
  sum(res^2)
}

# Values of the Objective function
RSS = matrix(
apply(grid, MARGIN = 1, FUN = my.func), nrow = p1n, ncol = p2n)

# Contour base
filled.contour(par1.vec, par2.vec, RSS, color.palette=terrain.colors)
# Align contour lines with the base
mar.orig = par("mar")
w = (3 + mar.orig[2]) * par("csi") * 2.54 
layout(matrix(c(2,1), nc = 2), widths = c(1,lcm(w)))
# Add Contour lines to the base
lev = pretty(range(RSS), 50) 
plot.new
contour(par1.vec,par2.vec,RSS, levels = lev,
    drawlabels = TRUE, lty = 2, axes = FALSE, frame.plot = FFALSE, add = TRUE)
title(xlab = expression(beta[1]), ylab = expression(beta[2]), main = "Contour Of RSS", sub = "puromycin.NL")

```

Based on the obtained contour of RSS, observe the better region (RSS close to 0), then zoom in (210, 0.07 as the initial guess)

```{r}
# Initial guess based on the contour plot
initial = list(par1=210, par2=0.07)
# Gauss-Newton is the default algorithm in R
parameter_each = capture.output({ puromycin.NL = nls(
Velocity~ par1*Concentration/(par2+Concentration),
start = initial, data = puromycin.df,
trace = TRUE) }) # with capture.output, we could get RSS and parameter in each iteration

model = puromycin.NL
model$m$deviance()
RSS(model)
```
```{r}
parameter_each # with capture.output, we could get RSS and parameter in each iteration
```

"Nonlinear regression uses an iterative algorithm to reduce the error sums of squares (SSE). For each iteration, the algorithm adjusts the parameter estimates in a manner that it predicts should reduce the SSE compared to the previous iteration. The iterations continue until the algorithm converges on the minimum SSE, "

```{r}
tmp = strsplit(parameter_each, " ") # Split according to a single white space
str(tmp)
# Coerce into a vector 
pei.vec = unlist(tmp)
pei.vec
n.itera = length(tmp); n.element = length(tmp[[1]])
index_1 = seq(4, by = n.element, length = n.itera) 
index_2 = seq(7, by = n.element, length = n.itera)
par1.vec = as.numeric(pei.vec[index_1]) # vector of the first parameter
par2.vec = as.numeric(pei.vec[index_2]) # vector of the second parameter
plot(par1.vec, type='o', xlab = "Index", ylab = "par1.vec", main = "beta_1", sub = "Velocity ~ par1 * Concentration/(par2 + Concentration)")
plot(par2.vec, type='o', xlab = "Index", ylab = "par1.vec", main = "beta_1", sub = "Velocity ~ par1 * Concentration/(par2 + Concentration)")

plot(puromycin.df$Concentration)
lines(puromycin.df$Concentration,predict(model),col="red",lty=2,lwd=3)
```

(? Puromycin.NL Fit not finished )

For this particular nonlinear model, it could be transformed into a linear model 

$\frac{\beta_1 c}{\beta_2 + c} = \frac{\beta_2}{\beta_1c}+\frac{1}{\beta_1}+\varepsilon^*$ not a same error now 
```{r}
Con.reciprocal = 1/puromycin.df$Concentration # get the reciporal of each variable 
Vel.reciprocal = 1/puromycin.df$Velocity
p.reciprocal.df = data.frame( Con.reciprocal = Con.reciprocal, Vel.reciprocal = Vel.reciprocal)
p.reciprocal.LM = lm(Vel.reciprocal~Con.reciprocal, data = p.reciprocal.df)

# !IMPORTANT! Back transform 
a.vec = p.reciprocal.LM$coefficients
paras.t.vec = c(1/a.vec[1], a.vec[2]/a.vec[1])
```

(? transformed graph not finished)


**Diagnostics**

For nonlinear regression, large sample size, the sampling distribution approaches normal 


*Wald test and Wald interval*

For hypothesis testing and confidence interval respectively 

in summary, t-statistic and p-values are those for Wald test, only meant to be used for a really big sample. 
```{r}
summary(puromycin.NL) 
```

*bootstrap*

used at least to check whether large-sample inference is appropriate. 
"construct the sampling distribution of the estimators in a nonlinear regression model, which then can be used to compute estimated standard errors, thus produce confidence intervals and prediction intervals.
"

用原有的X，把estimate的beta作为true parameter，并且error$\delta \sim^{i.i.d} Normal(0,\hat{\sigma}^2)$. Then generate $M$ sets of size-n samples of $Y_i$ based on the model with original $X$. Each of simulated dataset gives one $\beta$. Then the M samples of $\beta$ are used to approximate the sampling distribution of $\hat{\beta}$

```{r}
M = 10000 ## Bootstrapping
n = nrow(puromycin.df)
fvs = fitted.values(puromycin.NL)
vel.vec = fvs + # Generate the responses 
  rnorm(M*n, 0, summary(puromycin.NL)$sigma) # and \sigma 
vel.mat = matrix(vel.vec, nrow = M,
ncol = n, byrow = TRUE)

my.func = function(x){ 
  tmp.NL = nls(
    x~par1*Concentration/(par2+Concentration), 
    start = list(par1=210, par2=0.07),
    data = puromycin.df)
  tmp.NL$m$getPars()
}
parameter.sim = apply(vel.mat, MARGIN = 1, FUN = my.func)
p = 1
hist(parameter.sim[p,], probability = TRUE, xlab = bquote(hat(beta)[.(p)]), breaks = 30, main = bquote(paste(
"Bootstrap Sampling distribution of ", hat(beta)[.(p)])) )

tmp.x = seq(min(parameter.sim[p,]), max(parameter.sim[p,]), length = 100)
tmp.y = dnorm(tmp.x, puromycin.NL$m$getPars()[p], sqrt(vcov(puromycin.NL)[p,p]))
lines(density(parameter.sim[p,]), col = 2, lty = 2)
lines(tmp.x, tmp.y, col = 4, lty = 3)
legend("topright", lty = 2:3, col = c(2, 4), legend = c("KDE", expression(infinity)))
# ? what to get from this plot 
```

```{r}
# ?? 
# C.I. for parameters
quantile(parameter.sim[1,], probs=c(0.025, 0.975))

con.pred = 0.8 # C.I for conditional mean
fvs.sim = parameter.sim[1,]*con.pred/(parameter.sim[2,]+con.pred)
quantile(fvs.sim, probs=c(0.025, 0.975))
v.sim = fvs.sim + # Prediction interval 
  rnorm(M, 0, summary(puromycin.NL)$sigma)
quantile(v.sim, probs=c(0.025, 0.975))
```
Until now, we assumed that the errors following a normal distribution 
# L16 Logistic regression 
Mathematical equation / induction, see $Notes.md$
```{r}
credit.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/credit.csv", header = TRUE, sep = ",")
str(credit.df)
# visualize the data
# Scatter plot
index0 = (credit.df$default == 0L) 
index1 = (credit.df$default == 1L)
plot.new() # find default yes / no : two types 
plot(income~balance, type="n",  data = credit.df, xlab = "balance", ylab = "income", main = "Credit Default") + # type n: no display 
points(credit.df[index0, 3], credit.df[index0, 4], col = "deepskyblue", pch = 1) + points(credit.df[index1, 3], credit.df[index1, 4],
col = "chocolate", pch = 4) 
legend("topright", c("Default No", "Default Yes"), col = c("deepskyblue", "chocolate"),
pch = c(1, 4)) # error due to R markdown, should be fine in R 
# no difference caused by income, balance caused a difference 

```
```{r}
# Boxplot, further examine the difference caused by balance 
boxplot(balance~default, data = credit.df,
col = c("deepskyblue", "chocolate"), boxwex = 0.25, xlab = "Default",
ylab = "Balance", names = c("No", "Yes"), main = "Balance by Default")

# Mosaicplot->student more likely to default 
mosaicplot(
  table(list(
    Default = factor(credit.df$default, 
                     labels = c("No", "Yes")), 
    Student = credit.df$student)),
    col = c("deepskyblue", "chocolate"),
  main = "mosaicplot of Default by Student"
)
```


Use the logistic function

```{r}
# glm + binomial family, means logistic regression 
# logistic regression for one predictor 
credit.LG = glm(default~balance, family = binomial, data = credit.df)
coef(credit.LG)
newdata.df = data.frame(balance = 1000)
mhat = predict(credit.LG, newdata = newdata.df, type = "response") # response gives the probability 
mhat # same as exp(beta_0+beta_1 * 1000) / (1 + exp(beta_0 + beta_1 * 1000))
logit.credit = predict(credit.LG, newdata = newdata.df) # logit
(os = mhat/(1-mhat)) # odds
log(os) # log odds (=logit), so we could see how to get the response (mhat) from logit (log(os))
exp(logit.credit) / (1 + exp(logit.credit)) # = mhat (probability)
summary(credit.LG)
```
```{r}
# logstic regression for more than one predictors 
credit.all3.LG = glm(default~balance+income+student, family = binomial, data = credit.df)
summary(credit.all3.LG) 
# LR test of significance, similar to F-test
credit.null.LG = glm(default~1, family = binomial, data = credit.df)
LR.test = 2*(logLik(credit.all3.LG)[1] -logLik(credit.null.LG)[1]) # compare full with null model, so three beta reduced to zero, degree of freedom should be 3 
1-pchisq(LR.test, 3) # significantly strong evidence to reject that the reduced model is adequate

# LR test for reduced, similar to partial F-test
credit.no.income.LG = glm(default~.-income, family = binomial, data=credit.df) # must be the submodel (to the model going to be compared)
LR.test = 2*(logLik(credit.all3.LG)[1] -logLik(credit.no.income.LG)[1])

1-pchisq(LR.test, 1) # fail to reject, only reduce one model, degree of freedom is 1 
```
However, no way to check the validity of the model, and appropriateness of using asymptotic approximation, care with **small n** ! 

**Group dataset**

Group: means there are groups of data points that have the same $\mathbf{X}$
```{r}
# group logistic regression 
ingots.df = credit.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/ingots.csv", header = TRUE, sep = ",")
ingots.df # 20 binomial distributions 
ingots.LG = glm(notready/total~heat+soak, family = binomial, data = ingots.df, weights = total)
summary(ingots.LG)
```
```{r}
# check the residuals
# 1. Pearson Residual vs. fitted conditional probability 
pres = residuals(ingots.LG, type = "pearson")
fvs = predict(ingots.LG, type = "response") # response give the m, the probability 
plot(fvs, pres, xlab="Fitted Conditional Probability", ylab = "Pearson residuals", main="Residual Plot", sub = deparse(ingots.LG$formula))

# 2. Pearson Residuals vs. Logit 
fvs = predict(ingots.LG, type = "link") # logit 
plot(fvs, pres, xlab="Logit", ylab = "Pearson residuals", main="Residual Plot",sub = deparse(ingots.LG$formula))
```

**Simulation study**

for grouped dataset, shows finding the outliers / leverage point using residual plot, how the model change after removing those points. 
```{r}
# simulation study
set.seed(1)
n = 100 
x1 = rnorm(n, mean = 0, sd = 6)
x2 = rt(n, df = 1) # random student t distribution 
x3 = rbinom(n, size = 1, prob = 0.45)
beta0 = 0.6
beta1 = 0.3
beta2 = 0.1
beta3 = 0.1 
logit = beta0 + beta1*x1 + beta2*x2 + beta3*x3 
true.pi = exp(logit)/(1+exp(logit))
m = 100 
y = integer(n)
for (i in 1:n){
  y[i] = rbinom(1, size = m, prob = true.pi[i])
} # simulated y 
sim.df = data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, total = rep(m, n)) # simulated y
sim.LG = glm(y/total~x1+x2+x3, family = binomial, weights = total , data = sim.df)
res = residuals(sim.LG, type="pearson")
logit = predict(sim.LG) # default logit
prob = predict(sim.LG, type = "response") # Prob
plot(logit, res, xlab="Logit", ylab = "Pearson residuals", main="Residuals vs Estimated log odds", sub = deparse(sim.LG$formula))

plot(prob, res, xlab="Fitted Conditional Probability", ylab = "Pearson residuals", main="Residuals vs Estimated Pr[Y|X=1]", sub = deparse(sim.LG$formula))
plot(res, type="n",
ylab = "Pearson Residuals", main="y/total~x1+x2+x3")
text(logit); abline(h=0,lty=3) 
plot(logit[-c(83)], res[-c(83)], xlab="Logit", ylab="Pearson residuals") 

# what happens if exclude point index 83 ?
lines(smooth.spline(logit[-c(83)], res[-c(83)]), col = 2, lty = 2)
lines(smooth.spline(logit, res), col = 4, lty = 2)
legend("topright", c("83 in", "83 out"), col = c(2, 4), lty = 2)

# high leverage points 
head(sort(hatvalues(sim.LG), decreasing = TRUE)) # hatvalues: compute some of the regression (leave-one-out deletion) diagnostics for linear and generalized linear models 
# highest: 21

# influential points 
head(sort(cooks.distance(sim.LG), decreasing = TRUE))
# highest: 21 > 83

# 21: high leverage influential point, 83: influential outlier, exclude them in the final model 
sim.final.LG = glm(y/total~x1+x2+x3, family=binomial, weights = total, data=sim.df[-c(83, 21),])
# which gives the approximate input beta 0-3: 0.6, 0.3, 0.1, 0.1 


# R_d^2 is 
1-sim.final.LG$deviance/sim.final.LG$null.deviance
# =0.97, model capture most of the deviation in the date 
```


# L17 Poisson

```{r}
bomber.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/aircraft.csv", header = TRUE, sep = ",")
str(bomber.df)
# the MLE of beta 
bomber.PS = glm(Damage ~ Type + Month + Load, family=poisson, data=bomber.df)
bomber.PS


s1 <- data.frame(Type = 0, Month = mean(bomber.df$Month), Load = mean(bomber.df$Load))
predict(bomber.PS, s1)  # log of count 
        
exp(predict(bomber.PS, s1))
predict(bomber.PS, s1, type='response')  # count 
# the deviance 
1-bomber.PS$deviance/bomber.PS$null.deviance # 0.51: not a very good model, only explains 0.51 

# LR-test 
1-pchisq(bomber.PS$deviance,bomber.PS$df.residual) # 0.46: large p-value -> no evidence of lack of fit 

  # significance 
1-pchisq(bomber.PS$null.deviance,bomber.PS$df.null) # 0.0033: small p-value: at least one of the regressors is needed 

# for small sample & lack of distribution, do bootstrap (check the validity of using asymptotic approximation)
M = 100000 # bootstrap simulation
n = nrow(bomber.df)
fvs = fitted.values(bomber.PS) # mhat = mean 
dam.vec = rpois(M, lambda = fvs) # specify paramter of poisson to be the estimate, vector of means 
dam.mat = matrix(dam.vec, nrow = M,
ncol = n, byrow = TRUE)
my.func = function(x){
  tmp.PS = glm(x ~ Type + Month + Load,
              family=poisson, data=bomber.df) 
  return(c(tmp.PS$deviance, tmp.PS$null.deviance))
}

parameter.sim = apply(dam.mat, MARGIN = 1, FUN = my.func)
p = 1
hist(parameter.sim[p,], probability = TRUE, xlab = bquote(hat(beta)[.(p)]), breaks = 30, main = bquote(paste(
"Bootstrap Sampling distribution of ", hat(beta)[.(p)])) )
```
? poisson bootstrap 

```{r}
summary(bomber.PS)
```

**Model Selection**

1. get the models 
2. small datasize, use bootstrap simulation
3. at this case, determine based on the estimated MSE

```{r}
# select among these models 
bomber.PS = glm(Damage ~ Type + Month + Load, family=poisson , data=bomber.df)
bomber.Month.PS = glm(Damage ~ Month , family=poisson ,
data=bomber.df)
bomber.no.Type.PS = glm(Damage ~ Month + Load, family = poisson, data=bomber.df)
bomber.sq.PS = glm(Damage ~ Type + Month + poly(Load,2), family = poisson, data = bomber.df)
```


```{r}
# way to generate the above three models in more systematic way 
varname = names(bomber.df)
r.vec = varname[1]
p.vec = varname[-1]
k = length(p.vec)

my.func = function(x){
# Generate all combinations
combn(1:k , x, simplify = FALSE)
}
# Indices for all possible sub-models
index = unlist(
lapply(1:k, FUN = my.func), recursive = FALSE)

my.func = function(x) {
  # create a formula 
  as.formula(
        paste(r.vec[1],
             paste(p.vec[x], collapse = "+"),
             sep = "~")
  )
}
# Generate all the formulas for linear terms
formula.data = lapply(index, FUN = my.func)

# Create the formula for the poly term
poly = update(formula.data[[4]], ~.+poly(Load, 2))

# Final formula set, same as the formula we proposed before that want to select 
formula.vec = c(formula.data[c(7,2,6)], poly)

# number of models = 4
n.model = length(formula.vec)

my.func = function(x){
  # Run each model on bootstrap data
  tmp.mat = matrix(nrow = n, ncol = n.model) 
  for (i in 1:n.model){
    
    tmp.PS = glm(formula.data[[i]], family=poisson, 
                 data=bomber.df[x,])
    tmp.mat[, i] = predict(tmp.PS, type = "response", 
                           newdata = bomber.df)
  }
  return(colMeans((tmp.mat - bomber.df[,"Damage"])^2))
  # return mean square error for each model 
}

M = 10000
n = nrow(bomber.df) # size of original dataset 
index.mat = matrix(sample(1:n, size = M*n, replace = TRUE), 
                   nrow = M, ncol = n, byrow = TRUE)
mse.sim = apply(index.mat, MARGIN = 1, FUN = my.func) # M new data, MSE for each model of each new data 
rowMeans(mse.sim) # choose based on the estimated MSE of this bootstrap simulation

```

# L18 GLM
The GLM is a model with model structure 


# L19 GEE
```{r}
# It is clear that the measurement taken on the same device may be correlated.
semiconductor.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/semiconductor.txt", colClasses = c(rep("factor", 7), "numeric"), header = TRUE)
str(semiconductor.df, vec.len = 3)

# Cluster: fixed group 


library(gee)
semi.GEE = gee(Camber~.-Run, family = Gamma(link = "log"), id = Run, corstr = "AR-M", M = 1, data = semiconductor.df)
semi.GEE # the model does not give strong correlation, check glm

semi.GLM = glm(Camber~.-Run, family = Gamma(link = "log"), data = semiconductor.df)
summary(semi.GLM)

```
## Nonparametric 
### simple smoothing 

