---
title: "Applied Regression Analysis using R"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---
# L01-Basic 
use '_' in name, to avoid confusion with the member function
```{r}
# help
rm(list = ls()) # clean environment 
?is.logical()
library(help = "stats") # for function list in a package
is.integer(7L) # integer, with 'L' after number
is.integer(7)
x <- 1 # assignment 
1 -> x1
x = 1

# vector c()
month <- c('Jan','Feb','Dec')
class(month)
num <- 1:50 # return a vector 
num_seq <- seq(1, 50, by = 2) # create sequence: seq(from, to, by)
seq(stats::rnorm(20))
num_seq[which(num_seq>9)]

# categorical data
month.fac <- factor(month, order=TRUE, levels = c("Jan", "Feb", "Dec"))
month.fac.in <- factor(month) # default: mathematical order

# matrix, special vectors in R 
A = matrix(month, nrow = 6, byrow = TRUE, ncol = 9)
B = matrix(month, nrow = 6, byrow = FALSE, ncol = 9)

# list, combine different data type 
listAB = list(num = num_seq, Amatrix = A, Bmatrix = B)
class(listAB)

# data.frame
df.A <- data.frame(A)
df.A$X1
class(df.A)
class(df.A$X3)
df.A$X1 <- factor(df.A$X1)

# function
myfuc = function(x) { # key word function
    s = 0
    for (eps in x) {
      if (eps <= 0){
        next
      }
      s = s + eps
      if (s > 20) {
        break
      } 
    }
    s
}

# recycling rule
c(1, 2, 3, 4) + c(1, 2) # equal to c(1, 2, 3, 4) + c(1, 2, 1, 2)

```
e
## implicit coercion to mixed types
logical -> integer -> numeric -> complex -> character

as transform from logical to integer is easier

```c(11, month)```  will convert integer to a character 

# L02-slr
*simple linear regression basic* 

- regression analysis: statistical model that involve one dependent variable and one or more independent variables 

- regression: find a rule of picking distribution for *Y* from a space of infinitely many distribution that agrees with the data

 **Primary Assumption:** (for now) a sequence of random variables is independent and identically distributed (i.i.d.)
 
## Estimation: 

The process of using data to suggest a value for a parameter 

**Estimate:** the value suggested is called the estimate of the parameter (depend on data)

**Estimator:** a function of the data, that gives estimates of the parameter
$\hat{\theta} = h(X)$
, the distribution for $\hat{\theta}$ is called **sampling distribution** of the estimator.
```{r}
?pbinom
```
Given X~Normal(n, p)

Pr(X = x) = dbinom(x, size = n, prob = p)

Pr(X <= x) = pbinom(x, size = n, prob = p)

qbinom: The quantile is defined as the smallest value x such that F(x) ≥ p, where F is the distribution function. with p$\in$ [0,1], returns the smallest value of q s.t. Pr(X<=returnvalue) >= p

```{r}
dbinom(x, size, prob, log = FALSE)
pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
rbinom(n, size, prob)
```


### Binomial Example

*L02-16/55 pages, US presidents*

- if we become US presidents, we are more likely to have sons -- T/F depend on the p-value 
- US presidents are more likely to have sons -- independent of the p-value, because the current data has exhausted all the info of presidents

```{r}
rm(list = ls())  # clean working environment

## set up hypothesis
theta <- 1/2  ## assume equal chance 50-50
n <- 158  ## total number of trials
x <- 0:n  ## all possible number
x1 <- seq(0, n, by = 1)  # alternative way of set up sequence
fX <- dbinom(x, size = n, prob = theta)  ## pdf of x
# ? dbinom  ## getting help with ?
obs <- 67
p.lowerTail <- pbinom(obs, size = n, prob = theta)
## under the assumption of theta = 0.5
p.value <- 2 * p.lowerTail

## visualization
tmpL <- fX[x <= 67]  ## as extreme as what we observed
x.upperTail <- 1 + qbinom(p.lowerTail, 
                          size = n, prob = theta, lower.tail = FALSE) ## quantile function, different from pbinom 
tmpU <- fX[x >= x.upperTail]

M <- x[x > 67 & x < x.upperTail]
tmpM <- fX[x > 67 & x < x.upperTail]

plot(x, fX, type = "n", 
     xlab = "observed number of daughter", ylab = "probability mass")
lines(0:obs, tmpL,type = "h", col = "red")
lines(x.upperTail:n, tmpU,type = "h", col = "red")
lines(M, tmpM, tmpM,type = "h")
points(0:obs, tmpL, col = "red")
points(x.upperTail:n, tmpU, col = "red")
points(M, tmpM)
legend("topright", legend = "P-value", col = "red", lty = 1, pch = 1)
```

```{r}
## function to calculate the density of a binomial distribution
## modified with two more attributes: obs and n
myp_func <- function(n, obs, p){
  ## n: total number of binomial trials
  ## obs: observed number of events
  ## p: probability of success
  pbinom(obs, size = n, prob = p) - pbinom(obs - 1, size = n, prob = p)
  ## exactly the same as dbinom(obs, size = n, prob = p)
}

myp_func(190, 125, 0.5)
dbinom(125, size = 190, prob = 0.5)

myp_func(190, 125, 0.6)/myp_func(190, 125, 0.5)
dbinom(125, size = 190, prob = 0.6)/dbinom(125, size = 190, prob = 0.5)

## to find the maximum likelihood estimate based on the obs
## we need to find the p that maximise the underlying pdf/pmf
## myp_func is here to show how to write a simple function
## we will use the proper dbinom function here after


## to visually examine the mle of deep sea diver case study
## observations: 125 daughter of 190 children
pvec <- seq (0, 1, length.out = 10000000)  ## set the seq of all possible prob
lvec <- dbinom(125, size = 190, prob = pvec)  ## find the likelihood
plot (pvec, lvec, type = "l",  ## visualisation of the likelihood
      xlab = "True probability of having a daughter for a male deep sea diver",
      ylab = "Likelihood of observing 125 daughters of 190 children",
      main = "Likelihood fuction",
      xlim = c(-0.2, 1))

# common sense: best guess: the estimator of p is 125/190
# get from the maximum likelihood function
phat <- pvec[which.max(lvec)]
abline(v = phat, col = "red")
legend("topleft", legend = "Maximum Likelihood Estimate",
       lty = 1, col = 2)
# get that phat = 0.658...

```
### Confidence Interval
Confidence interval of binomal distribution 
- When says 

We are 95\% confident that the true probability of having a daughter is between (0.586, 0.725) for a male deep-sea diver 

it is actually means 

The method used to obtain the interval (0.586, 0.725) will contain the true probability 95\% of time if it is to be repeated 
```{r}
binom.test(125, n = 190, p = 0.5) 
```
```{r}
binom::binom.confint(125, n = 190)
```

```{r}
rm(list = ls())
num = 1e3
n = 190
runif(1)
```

## Law of large numbers 
Suppose that $X_1,X_2,.. X_n$ all have the same expected value $\mathbb{E}[X_i]=\mu$, the same variance $Var[X_i] = \sigma^2$ and zero covariance with each other $Cov[X_i, X_j] = 0$, when $i\neq j$

or could say if $X_i$ are i.i.d., then the following holds 
$$\bar{X_n} = \frac{1}{n}\sum_{i=1}^nX_i \rightarrow \mu  ,when \quad n\rightarrow \infty $$

reduce the spread of the distribution to 0 effectively 
$$\lim_{n\rightarrow \infty} Pr(|\bar{X_n} - \mu| > \varepsilon) = 0, \rm{for \ any} \ \varepsilon > 0 $$

### Illustration of LLN
As n increases, the mean of X approaches the true mean and the error approaches 0.
```{r}
rm(list = ls())
n <- 1e4
lambda <- 3

xpois <- rpois(n, lambda)  ## consider poisson random variable
xexp <- lambda  ## true underlying mean

nseq <- 1:n
# investigate Xbar when n increases, `cumsum` is  cumulative sum 
xcbar <- cumsum(xpois)/nseq  ## sample mean as n increases, at various stage 
error <- abs(xcbar - xexp)  ## relative error
plot(nseq, xcbar, type = "l", xlab = "Sample Size",
     ylab = "Sample Mean")
abline(h = xexp, col = 2)
legend("topright", legend = "True mean", lty = 1, col = 2)


plot(nseq, error, type = "l", xlab = "Sample Size", ylab = "Relative Error")
abline(h = 0, col = 2)
```
A better look:
```{r}
## a better look at LLN
sample.size.vec <- c(5, 10, 50, 100, 500)  ## 5 cases
ncases <- length(sample.size.vec)
num <- 1e3  ## number of repetitions
xbar.vec <- double()  ## x bar for all simulations
error.vec <- double()  ## error for all simulations
n.vec <- integer()  ## sample size for each case

for(j in 1:ncases){
  n <- sample.size.vec[j]  ## current sample size
  s.vec <- double(num)  ## all x bar for this n
  e.vec <- double(num)  ## all error for this n

  for (i in 1:num){  ## repeat num number of times
    x <- rpois(n, lambda)  ## sample of x by sample size 5, 10 etc.
    s.vec[i] <- sum(x)/n
    e.vec[i] <- abs(s.vec[i] - xexp)/abs(xexp)
  }
  
  xbar.vec <- c(xbar.vec, s.vec)
  error.vec <- c(error.vec, e.vec)
  n.vec <- c(n.vec, rep(sample.size.vec[j], num))
  }

x.df <- data.frame(xbar = xbar.vec, error = error.vec, n = n.vec)
boxplot(xbar~n, data = x.df, xlab = "Sample Size", ylab = "Sample Mean")
abline(h = xexp, col = 2)
legend("topright", legend = "True mean",
       lty = 1, col = 2)

boxplot(error~n, data = x.df,
        xlab = "Sample Size", ylab = "Relative Error")
abline(h = 0, col = 2)
## as we increase the sample size
## the error gets closer to 0 and more constant

```



## Central Limit Theorm 

![](/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/image/CLT.png)
### Illustration 
```{r}
sample.size.vec
lambda; xexp

xvar <- lambda  ## true variance for Poisson
sqrt.n.vec <- sqrt(n.vec)  ## last for loop

sample_error <- xbar.vec - xexp
sample_variance <- sqrt(xvar)
r.vec <- sqrt.n.vec*sample_error/sample_variance  ## normalised error

x.hist <- seq(-4, 4, length.out = 100)
par(mfrow = c(2, 3))

for(eps in sample.size.vec){
  ss <- r.vec[n.vec == eps]  ## subset by n
  tname <- bquote(bold(atop("Sample Size = "~.(eps), "1000 Repetition")))
  
  hist(ss, feq = FALSE, xlab = "r", main = tname,
       ylim = c(0, 0.6), xlim = c(-4, 4))

  lines(density(ss), col = 4, lty = 2)  ## kernel denity estimation
  lines(x.hist, dnorm(x.hist), col = 2, lty = 1)  ## true density function
  legend("top", col = c(1, 2), lty = c(2, 1),
         legend = c("Estimation", "True"))
}

true.norm <- rnorm(num, mean = 0, sd = 1)
hist(true.norm, freq = FALSE, ylim = c(0, 0.6),
     xlim = c(-4, 4), xlab = "Z", main = "100 Z~N(0, 1)")
lines(x.hist, dnorm(x.hist), col = 2, lty = 1)
par(mfrow = c(1, 1))

sample_quantile_5 = sort(r.vec[n.vec == 5])
sample_quantile_10 = sort(r.vec[n.vec == 10])
sample_quantile_50 = sort(r.vec[n.vec == 50])
sample_quantile_100 = sort(r.vec[n.vec == 100])
sample_quantile_500 = sort(r.vec[n.vec == 500])
sample_cdf = (1:num)/num
plot(sample_quantile_5, sample_cdf, type = "s",
     xlab = "", ylab = "",
     main = "Cumulative distribution function n = 500")
lines(sample_quantile_10, sample_cdf, col = 3, lty = 4)
lines(sample_quantile_50, sample_cdf, col = 4, lty = 4)
lines(sample_quantile_100, sample_cdf, col = 5, lty = 4)
lines(sample_quantile_500, sample_cdf, col = 6, lty = 4)
lines(x.hist, pnorm(x.hist), col = 2, lty = 2)
legend("topleft", lty = c(1, 2), col = c(1, 2), 
       legend = c("Estimation", "True CDF"))


```


# L03 - LSE & MLE

$MSE(m) = \mathbb{E}[(Y-m)^2] = Var[Y] + (\mathbb{E}[Y-m])^2$ 

-> bias variance decomposition

The common model assumptions for slr 

bias of an estimator $\hat{\theta}$, $\theta$ is a fixed population parameter 
$Bias(\hat{\theta},\theta) = \mathbb{E}[\hat{\theta} - \theta]$

- residual 

the estimate $\hat{e_i}$ is known as **residual** .

observed $y_i$ and $x_i$,
$\hat{e_i} =y_i - ( b_0 + b_1x_i)$, where $b_0$ and $b_1$ are estimates of $\beta_0$ and $\beta_1$; $\hat{y_i} = b_0 + b_1x_i$ is known as the predicted/fitted value. 

Then  $y_i = \hat{y_i}+\hat{e_i}$

- consistent 

An estimator is **consistent** if
$$\hat{\theta_n} \rightarrow \theta \quad \rm{when} \quad n\rightarrow \infty$$
### Least Square Estimation 

$SS_E$: error sum of squares 

$SS_E:=e_1^2 + e_2^2 + ...+e_n^2 = \sum_{i=1}^n(y_i-(b_0+b_1x_i))^2$

## model assumption for slr
![](/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/image/modelAss.png)

## assumptions about the error term 



# Overview of statistics 
In true universe, we have random variable $Y$;

In real world, we observe value $y$;

Then to link them together, we use **estimator**. However, error exist in this process, two types 

- sampling error (improve when data collection)
- estimation error (improve using statistics）

Two main things 
- parameter estimation 
- hypothesis testing 

To test the model sufficiency 

- F-Test for significance of regression: whether all predictors are zero 
- T-test: whether a single predictor is necessary for a given model
- Partial F-test: can be applied to an arbitrary subset of predictors 




# L04 - diagnostics



## residual plot 

$\hat{e_i}\quad vs. \quad \hat{y_i}$ fitted value of y

the variance of $\varepsilon$ in true world is expected to be a constant, the variance of the residuals could be shown it is also a constant. 


To check the assumptions of the residual

- 1. In residual plot, it should be random scatter, no underlying non-linear relationship
- 2. In residual plot, randomly scatter around mean zero, with basically unchanged variance 
- 3. check they are independent of each other by ACF 
- 4. check normality by QQplot (Quantile-Quartile normal plot)




## autocorrelation function
If errors are independent, the estimates should be small and patternless 

time series variable - very likely to be underlying correlation among residuals 

do the residual plot after fit the tesla price model

observe some high price - after days are likely to be large

based on ACF plot: determine the underlying lag correlation 

bad impact - negative on the following days 

tesla price today ~ tomorrow? ~ next week? :**autocorrelation function**

hypothesis -> colloect data -> describe the data use (sample mean/.../plot) -> fit the model -> use diagnostics to check the assumption (through residual ~ plot residual vs. fitted value / residuali vs. residualj 或者 autocorralation check coevariance / residuals vs. X) --- if the assumption is violated -> do the transformation ( constant variance  ~ log trandformation of y / normality -> box-cox to find additional variable, 可以同时对于y与x transform) 



#### Check normality 
plotting the estimated Vs assumed distribution

The error should follow normal distribution N(0, $\sigma^2$)

```{r}
oc.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/old_car.txt", header = TRUE)
str(oc.df)
age = 91 - oc.df$year
oc.LM = lm(price~age, data = oc.df)
model = oc.LM

## Residual vs. fitted values 
plot(fitted.values(model), residuals(model), xlab = "Fitted values", ylab = "Residuals", main = "Residual Plot", sub = "lm(price~age)")
```


# L05 - transformation

$Y_i|X_i$ ~ Normal $(\beta_0+
\beta_1x_i, \sigma^2)$

## Box-Cox transformation 
![](/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/image/boxcox.png)


When y and x both do not follow normal dist., use transformation 
find the $\lambda$ that maximize the likelihood function

如果log transformation是可以的，则会得到lambda的范围基本在0

而都不可以的情况，可以用提供的box-cox，可以同时转化x和y
```{r}
library(MASS)
?boxcox
```


```{r}
oc.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/old_car.txt")
oc.LM = lm(price~age, data = oc.df)
```
# L06 Prediction 
to predict based on the current data 


## TSS
variability in the response variable 

$TSS = \sum^n_{i=1}(y_i-\bar{y})^2=\sum^n_{i=1}(\hat{y_i}-\bar{y})^2 + \sum^n_{i=1} \hat{e_i}^2$
total sum of squares = explained sum of squares + residual sum of squares 

TSS = ESS + RSS 

# L07 Multiple linear regression


![](/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/image/multi.png)

 regression coefficients in multiple regression: partial regression coefficients, as their interpretation requires other variables should be held fixed. 

**How least square works**

$$
\begin{align*}
\textbf{y}  &= \mathbf{X}\hat{\beta} + \hat{e}\\
\rm{minimize\ residual\ sum\ of\ squrares} &\quad\hat{e}^T\hat{e}\\
\rm{obtain\ the\ estimator}&\quad \hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\\
\rm{fitted\ value}&\quad \hat{\textbf{y}}  = \mathbf{X}\hat{\beta}= \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}=\mathbf{P}\mathbf{y} \\
\rm{residual}& \quad \hat{e}=\mathbf{y} -\hat{\mathbf{y}}=(\mathbf{I}-\mathbf{P})\mathbf{y}
\end{align*}
$$
Projection Matrix$\mathbf{P}, \mathbf{P}^T=\mathbf{P}, \mathbf{P}^2=\mathbf{P}$ 

Identity Matrix$I$

unbiased $\mathbf{\hat{\beta}}, \mathbf{\hat{\beta}}\sim\rm{Normal}(\beta,\sigma^2(\mathbf{X^T \mathbf{X}})^{-1})$

Case study: remember the small data set, we do not have strong evidence \
three variables: $y$ as gain; $X$ as driven-in time and dose 

1. drive-in time or dose influences gain linearly O -> F-test \
2. drive-in time influences gain for a given dose linearly -> t-test \
3. dose influences gain for a given drive-in time linearly -> t-test \
4. there is any reason to use both drive-in time and dose to explain  gain -> compare with the submodel using adjusted R-squared 

```{r}
# install.packages("xlsx")
library(xlsx)
trans.df = read.xlsx("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/transistor.xlsx", sheetIndex = 1)
## full model first, take all the regression variables first.
trans.LM = lm(gain~., data = trans.df) # use gain as response, '.' means all other variables are x

plot(trans.LM, which = 1) ## ?
model = trans.LM
fvs = fitted.values(model)
res = residuals(model)
sres = rstandard(model) # standardised 
## before make inference, need to check the assumptions first
# -- residual plot --#
plot(fvs, sres, xlab = "Fitted Values", ylab = "Standardized Residuals", main = "Residual Plot", sub = "lm(formula=gain~.,data = trans.df)") + abline(h = 0, lty = 2, col = "red") # use standardized residuals 
# no indication of nonlinearity, correlation, non-constant variance 

plot(res[-nrow(trans.df)], res[-1], xlab = "Previous", ylab = "Residuals", main = "Residuals vs. Previous Residual", sub = "lm(formula=gain~.,data = trans.df)")
acf(res)
# From both plots, no indication that the residuals are correlated 

qqnorm(res)
qqline(res, col = 2, lty = 2) # right in the R script 

# shapiro-wilk
shapiro.test(res) # small p-value: reject 

## no obvious violation of the assumptions, so we could proceed
summary(model)
```
For question 4, using adjusted R squared which takes the model complexity into account. However, it is only meaningful when **all the model assumptions are met**. 
```{r}
#full model
trans.LM = lm(gain~., data = trans.df)
# submodel 
trans.dose.LM = lm(gain~dose, data = trans.df)
trans.dit.LM = lm(gain~dit, data = trans.df)
# should verify through residual plots then see the summary and compare the adjusted R square
res.dose = residuals(trans.dose.LM)
plot(res.dose)
acf(res.dose)
qqnorm(res.dose) # violate the assumption
shapiro.test(res.dose)

# why
res.dit = residuals(trans.dit.LM)
plot(res.dit)
acf(res.dit)
qqnorm(res.dit) 
shapiro.test(res.dit)

# all violate the assumption, because of the samll dataset, need to be more strict
# summary(trans.dit.LM) summary is meaningless 
```


# L08 - poly
Example1: only drop variables after check residual plots (all the assumptions are satisfied)

```{r}
sim.df = read.csv("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/sim_poly_class.csv.bz2")
str(sim.df)
sim.LM = lm(y~x1+x2+x3, data = sim.df)
# plot the residuals, find non-random, indeicate nonlinearity
# The nonlinearity would be clearer if add estimated conditional mean to the residual plot 
c.mean.plot = function(tmp.x, tmp.y, n.each, pos = "topleft"){ n.x = rounod(length(unique(tmp.x)) / n.each)
x.group = cut(tmp.x, breaks = n.x, labels = FALSE)
x.vec = tapply(tmp.x, x.group, mean) y.vec = tapply(tmp.y, x.group, mean)
points(x.vec, y.vec, col = "blue", pch = 16) lines(x.vec, y.vec, col = "blue", lty = 2)
legend(pos, "Estimated Conditional Mean", col = "blue", lty = 2, pch = 16)
}

# then do the plot: standardised residual against one of the regressors
# based on the plot, have a new model sim.quad.LM = lm(y~x1+x2+x3+I(x2^2),data = sim.df), then check the assumptions 
# when all assumptions meet, look at the summary for each test 
```


str(LifeCycleSavings)
```{r}
density(~sr, data = LifeCycleSavings)

## Forward, begin with an empty model 



## use anova to do the F-test


## because of the negative correlation between pop15 and pop75, and the small variance of pop75 itself, add one pop75 do not add much information 

## dpi's variance close to infinity (random actually), so still not add much information


## 2和5比较   without pValue -> 2,5 is submodel of full model, 所以2和4可以比较 5和4可以比较 but we could not compare 2 and 5 using F-test (no nested relationship between them)

## model 4 residual plots 
## residual vs. dpi -> very few in 3000-4000 (outliers / points with high leverage / influential points (will influence the regression much   ))


## cook's distance : look at the specific points and determine based on the target whether need to exclude 


## box-cox plot -> reduce the high leverage points (reduce variance in x)

## fit5 
```
need normally distributed y but do not need normally distributed x, because error term relates to y 
two mode / clear gap in the residual plot -> two distributions 

当ACF的蓝色线很宽的时候，说明没有太多的data

# L10 

- stability problem 
```{r}
set.seed(1)
x.vec = rnorm(10, 0, 1)
z.vec = 10*x.vec
tmp = seq(-3, 3, length.out = 10)
y.vec=rnorm(10, mean=x.vec, sd=3)
plot(tmp, 2*tmp, type = "n")
lm(y.vec~x.vec-1) # make the intercept not considered 
replicate(20, {y.vec=rnorm(10, mean=x.vec, sd=3)     abline(lm(y.vec~x.vec-1))})
```

# L 11 
Case Study
shows how to address multicollinearity and unusual points (data cleaning) 
```{r}
chem_pro.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/chem_pro.csv", sep = ",", header = TRUE)
# check 
str(chem_pro.df) # different data type, might be a typo for ratio's type

# convert it to double 
chem_pro.df$ratio = as.character(chem_pro.df$ratio)
chem_pro.df$ratio = as.double(chem_pro.df$ratio)
chem_pro.df$ratio

summary(chem_pro.df) # more test 
# boxcox plot useful to identify unusual values

par(mfrow = c(1,4))
lapply(chem_pro.df, boxplot)
par(mfrow = c(1,1))
# data cleaning result: unusual point in variable 2 and one in variable 4
# So we modify the 44th observation and keep record of it
conversion_typo = which( chem_pro.df$conversion <= -10)
chem_pro.df$conversion[conversion_typo] = - chem_pro.df$conversion[conversion_typo] # then run boxcox again will find usual plot 

# pairs plot 
## put histograms on the diagonal, don't know how to show the number 
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
pairs(chem_pro.df, panel = panel.smooth,
      cex = 1.5, pch = 24, bg = "light blue", horOdd=TRUE,
      diag.panel = panel.hist, cex.labels = 2, font.labels = 2)


model = lm(yield~conversion + flow + ratio, data = chem_pro.df) # howecer we truely want to check 1/ratio
# then check the residual plot vs. fitted value / each regressor (conversion/ratio/flow)
# check independece of residuals, ACF/ previous residual vs. 
#  normqq / small data size, do shapiro.test check normality 

# stabillity problem 
# VIF (stability problem)
Xminus1 = chem_pro.df[, -1]
VIF = diag(solve(cor(Xminus1))) # inverse of diagonal 
VIF #  no larger than 10, no multicollinearity 

# unusual points (influence stability problem)
  # 1. method: pii 
pii.vec = hatvalues(model) # pii: leverage score 
order(pii.vec, decreasing = TRUE)
  # 2. method: influence measure 
im = influence.measures(model)
im # Report and possibly delete influential points
# high leverage, not mean oulier. near regression line: not outlier 

#### check the first theory:Chemical theory predicts yield is related to the reciprocal of ratio
recip.LM = lm(yield~ conversion + flow +
+ I(1/ratio), data = chem_pro.df)
summary(recip.LM) # then have siginificace 1/ratio 
sres = residuals(recip.LM)
recip_outlier_index = which( sres < -2.5 )
# then check the residual plot vs. fitted value / each regressor (conversion/ratio/flow) -> find residual vs. flow need to be trandformed
# -> residual vs. ratiro: curvature due to high leverage point 
recip_hl_index = which(1/chem_pro.df$ratio > 25)
recip_hl_index
ratio_unusual
chem_pro.df[ratio_unusual,]
# check independece of residuals, ACF/ previous residual vs. 
#  normqq / small data size, do shapiro.test check normality 


#### check the second theory: predicts yield is related to conversion*flow (interaction)

prod.LM =lm(yield~ conversion + poly(flow,3) + I(1/ratio) + I(flow*conversion),
data = chem_pro.df)
prod.res = residuals(prod.LM) # check residuals for the assumptions, seems be satisfied 
summary(prod.LM) # do not support interaction 

# check VIF for those 
conversion = chem_pro.df$conversion
flow = poly(chem_pro.df$flow,3)
recip_ratio = 1/chem_pro.df$ratio
prod = chem_pro.df$flow * chem_pro.df$conversion
Xtd = cbind(conversion, flow, recip_ratio, prod)
VIF = diag(solve(cor(Xtd)))
VIF # more than 10  # do not support interaction 


# move back to the cubic model 
cubic.LM = lm(yield~ conversion + poly(flow,3) + + I(1/ratio), data = chem_pro.df)
# and investigate the e↵ect of unusual points on the model cubic.LM.

```

# L12
Heteroskedasticity
```{r}
cleaning.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/cleaning.txt", header = TRUE)
summary(cleaning.df)
cleaning.LM = lm(Rooms~Crews, data = cleaning.df)
crews.group = factor(cleaning.df$Crews)
tapply(cleaning.df$Rooms, crews.group, length) # repeated observations 
convar = tapply(cleaning.df$Rooms,crews.group,var)
convar # conditional sample variance of the response 
v.vec = convar[crews.group] # vector of corresponding conditional variance
w.vec = 1/v.vec # vector of diagonal elements of w
cleaning.WLS = lm(Rooms~Crews, weights = w.vec, data=cleaning.df) # give the desired estimate of \hat{\beta_w}

# then the residual analysis should be done with  \hat{e_w}=W^{1/2}\hat{e}
model = cleaning.WLS
res = residuals(model)
resw = sqrt(w.vec) * res
```

as the $\hat{e_i}$ might be very small/large, work with 

$z_i=ln(\hat{e_i}^2)=2ln|\hat{e_i}|$, the log-scale provides extra numerical stability 


```{r}
cvrv.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/cvrv.csv", sep = ",", header = TRUE)
cvrv.LM = lm(capital~rental, data = cvrv.df)
cvrv.res = residuals(cvrv.LM)
plot(fitted.values(cvrv.LM), cvrv.res) # the variance is clearly unequal, and rental varies continuously., not in the same scale

z = 2 * log(abs(cvrv.LM$residuals)) #  z is the auxiliary response,
auxiliary.LM = lm(z~rental, data = cvrv.df)  # Perform the auxiliary regression
v.vec = exp(auxiliary.LM$fitted.values) # transform back to have the estimated \sigma 
w.vec = 1/v.vec
cvrv.WLS = lm(capital~rental, weights = w.vec, data = cvrv.df) # weight according to the estimated \sigma 
```

# L13
correlated errors, check of independence 
```{r}
sales.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/sales.txt",  sep = "", header = TRUE)
sales.LM = lm(sales~advertising, data = sales.df)
```
 add lag do not help 
 
 fit the residuals (among previous and current residuals)
 
 $\varepsilon_i = p\varepsilon_{i-1} + v_i$, the true error will add the new $v_i$. So mainly useful for prediction. Bias and variance should be reconsidered. 
 
 $v_i$ is normal, so  $\varepsilon_i$  should be normal 
 
 structure : first-order autoregressive process / AR(1) (important in time series)
 
 **improve the model by fitting the residuals with time series issue**
 
 
# L14 variable selection


```{r}
evap.df = read.table("/Users/yuxinmiao/Documents/JI/JI2020Fall/VE406/R code/in-class demo/Data/evap.txt", header = TRUE)
# install.packages("leaps")

# try to exhaust all the combinations, use RSS to determine. only look at which term is significant or not: for explain but not prediction 
regsubsets.out = leaps::regsubsets( evap~., data = evap.df, nbest=1, nvmax = NULL,method = "exhaustive")  # nbset=1: 1 best model for each p
# NULL for no limit on p


# before using any of the method to pick up the best model, we should first check the model assumption
# when made transformation on regressors, t-test or partial F test should not be used 
# transform the response : redo the variable selection ! 



summary(regsubsets.out) # star means regressor that needed 



plot(regsubsets.out) # related to each of the independent variables 
plot(regsubsets.out, scale = "r2", main = "R^2") # the darker, the better 
plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2")
plot(regsubsets.out, scale = "Cp", main = "Mallow’s Cp")
plot(regsubsets.out, main = "BIC")
```

The validity of the model is a priority especially when the goal is **to explain**.


**to predict, different modern approach**: data splitting / cross-validation / bootstrapping 


to regulazition: 

**ridge regression:** shrinks the number of coefficients by imposing a penalty, encourages the coefficients to be zero. higher $\lambda$, more penalization. drop the inefficient coefficient. L2norm regulazition 
*do the normal transformations to it*

**lasso regression:** 
