################################################################################
## class notes for ve406 - 03
## writtenn by: tong zhu
## date: sep 15


################################################################################
## function to calculate the density of a binomial distribution
## modified with two more attributes: obs and n
myp_func <- function(n, obs, p){
  ## n: total number of binomial trials
  ## obs: observed number of events
  ## p: probability of success
  pbinom(obs, size = n, prob = p) - pbinom(obs - 1, size = n, prob = p)
  ## exactly the same as dbinom(obs, size = n, prob = p)
}

myp_func(190, 125, 0.5)
dbinom(125, size = 190, prob = 0.5)

myp_func(190, 125, 0.6)/myp_func(190, 125, 0.5)
dbinom(125, size = 190, prob = 0.6)/dbinom(125, size = 190, prob = 0.5)

## to find the maximum likelihood estimate based on the obs
## we need to find the p that maximise the underlying pdf/pmf
## myp_func is here to show how to write a simple function
## we will use the proper dbinom function here after


################################################################################
## to visually examine the mle of deep sea diver case study
## observations: 125 daughter of 190 children
pvec <- seq (0, 1, length.out = 10000000)  ## set the seq of all possible prob
lvec <- dbinom(125, size = 190, prob = pvec)  ## find the likelihood
plot (pvec, lvec, type = "l",  ## visualisation of the likelihood
      xlab = "True probability of having a daughter for a male deep sea diver",
      ylab = "Likelihood of observing 125 daughters of 190 children",
      main = "Likelihood fuction",
      xlim = c(-0.2, 1))

phat <- 125/190  ## common-sense tells us that this is our best guess
## or let's find out from the likelihood
phat <- pvec[which.max(lvec)]
abline(v = phat, col = "red")
legend("topleft", legend = "Maximum Likelihood Estimate",
       lty = 1, col = 2)
## ggplot2 can be used to improve the aesthetic of the plots
## for the classroom demo, we will use the base plot function
## as installation of ggplot2 may be limited in some cases
## e.g. using R in a secured environment when you have not 
## authority to install package


################################################################################
## confidence interval of binomial distribution
## either a normal approximation
## or the exact solution: Clopper-Pearson
binom.test(125, n = 190, p = 0.5)

install.packages("binom")
binom::binom.confint(125, n = 190)
## exact: clopper-pearson
## prop.test: normal approximation


################################################################################## another simulation study on CI, estimation, and testing
##  but this time with a known true parameter
rm(list = ls())
num <- 1e4  ## number of repeated observations
n <- 190  ## number of trials
p <- runif(1)  ## a true parameter but generated by random
x <- 0:190  ## all possible events
fx <- dbinom(x, size = n, prob = p)  ## density of all possible events

## let's first take a look at the density of all possible events
plot(x, fx, type = "h",
     main = "The True Density Function")
points(x, fx, pch = ".", col = "blue", cex = 3)
x_unlikely_lower <- qbinom(0.025, size = n, prob = p)
yL <- dbinom(x_unlikely_lower, size = n, prob = p)
arrows(x0 = 0, y0 = yL,
       x1 = x_unlikely_lower, y1 = yL,
       angle = 15, col = 2, code = 3, lwd = 1.2)
x_unlikely_upper <- qbinom(0.025, size = n, prob = p, lower.tail = FALSE)
yU <- dbinom(x_unlikely_upper, size = n, prob = p)
arrows(x0 = x_unlikely_upper, y0 = yU,
       x1 = n, y1 = yU,
       angle = 15, col = 2, code = 3, lwd = 1.2)
legend("topleft", "Extreme at 5% level under the true p", x.intersp = 0)
par(font = 5)
legend("topleft", legend = NA, lwd = 1, lty = NA, pch = 171, col = 2,
       bty = "n", pt.cex = 1.3)
par(font = 1)

## what are the feasible observations we mostly likely to observe 
## under the true p
X <- rbinom(num, size = n, prob = p)  ## random events based on the true p
counts <- table(X)  ## table of counts by frequency
nonzero.counts <- names(counts)  ## find all non-zero counts
head(nonzero.counts)
tmp <- as.character(x_unlikely_lower)  ## to locate the lower bound
xL.index <- which(nonzero.counts == tmp)
tmp <- as.character(x_unlikely_upper)  ## to locate the upper bound
xU.index <- which(nonzero.counts == tmp)
tmp <- length(counts)
index.vec <- rep(1, tmp)  ## set the colour scheme for the middle interval
index.vec[1:xL.index] <- 2  ## set the colour scheme for the lower bound
index.vec[xU.index:tmp] <- 2  ## set the colour scheme for the upper bound

cols.vec <- c("grey", "red")[index.vec]
x.mean <- round(n*p)
xm.index = which(nonzero.counts == x.mean)
barpos <- barplot(counts, col = cols.vec,
                  xlab = "observed number of daughters",
                  ylab = "Frequency",
                  main = "Number of daughters mostly likely to be observed")
text(barpos[xm.index], counts[[xm.index]] + 5,
     bquote(mu~"="~.(x.mean)),
     col = 4, srt =90)
legend("topright", legend = "Extreme at 5% level\nunder the true p",
       fill = 2, bty = "n")

p
## run exact binomial test
res <- binom.test(125, n = n, p = p)
names(res)
res$p.value
res$conf.int
res.df <- data.frame(CIL = double(), CIU = double())
for(i in 1:num){
  res = binom.test(X[i], n = n, p = p)
  res.df[i, 1:2] <- res$conf.int[1:2]
}

n_ci_contain_true <- sum(res.df[, 1] < p & res.df[, 2] > p)
rate <- 1 - n_ci_contain_true/num

n_ci_contain_true/num


################################################################################# law of large numbers
rm(list = ls())
n <- 1e4
lambda <- 3

xpois <- rpois(n, lambda)  ## consider poisson random variable
xexp <- lambda  ## true underlying mean

nseq <- 1:n
# investigate Xbar when n increases, `cumsum` is  cumulative sum 
xcbar <- cumsum(xpois)/nseq  ## sample mean as n increases, at various stage 
error <- abs(xcbar - xexp)  ## relative error
plot(nseq, xcbar, type = "l", xlab = "Sample Size",
     ylab = "Sample Mean")
abline(h = xexp, col = 2)
legend("topright", legend = "True mean", lty = 1, col = 2)


plot(nseq, error, type = "l", xlab = "Sample Size", ylab = "Relative Error")
abline(h = 0, col = 2)

## a better look at LLN
sample.size.vec <- c(5, 10, 50, 100, 500)  ## 5 cases
ncases <- length(sample.size.vec)
num <- 1e3  ## number of repetitions
xbar.vec <- double()  ## x bar for all simulations
error.vec <- double()  ## error for all simulations
n.vec <- integer()  ## sample size for each case

for(j in 1:ncases){
  n <- sample.size.vec[j]  ## current sample size
  s.vec <- double(num)  ## all x bar for this n
  e.vec <- double(num)  ## all error for this n

  for (i in 1:num){  ## repeat num number of times
    x <- rpois(n, lambda)  ## sample of x by sample size 5, 10 etc.
    s.vec[i] <- sum(x)/n
    e.vec[i] <- abs(s.vec[i] - xexp)/abs(xexp)
  }
  
  xbar.vec <- c(xbar.vec, s.vec)
  error.vec <- c(error.vec, e.vec)
  n.vec <- c(n.vec, rep(sample.size.vec[j], num))
  }

x.df <- data.frame(xbar = xbar.vec, error = error.vec, n = n.vec)
boxplot(xbar~n, data = x.df, xlab = "Sample Size", ylab = "Sample Mean")
abline(h = xexp, col = 2)
legend("topright", legend = "True mean",
       lty = 1, col = 2)

boxplot(error~n, data = x.df,
        xlab = "Sample Size", ylab = "Relative Error")
abline(h = 0, col = 2)
## as we increase the sample size
## the error gets closer to 0 and more constant


################################################################################# central limit therom
sample.size.vec
lambda; xexp

xvar <- lambda  ## true variance for Poisson
sqrt.n.vec <- sqrt(n.vec)  ## last for loop

sample_error <- xbar.vec - xexp
sample_variance <- sqrt(xvar)
r.vec <- sqrt.n.vec*sample_error/sample_variance  ## normalised error

x.hist <- seq(-4, 4, length.out = 100)
par(mfrow = c(2, 3))

for(eps in sample.size.vec){
  ss <- r.vec[n.vec == eps]  ## subset by n
  tname <- bquote(bold(atop("Sample Size = "~.(eps), "1000 Repetition")))
  
  hist(ss, feq = FALSE, xlab = "r", main = tname,
       ylim = c(0, 0.6), xlim = c(-4, 4))

  lines(density(ss), col = 4, lty = 2)  ## kernel denity estimation
  lines(x.hist, dnorm(x.hist), col = 2, lty = 1)  ## true density function
  legend("top", col = c(1, 2), lty = c(2, 1),
         legend = c("Estimation", "True"))
}

true.norm <- rnorm(num, mean = 0, sd = 1)
hist(true.norm, freq = FALSE, ylim = c(0, 0.6),
     xlim = c(-4, 4), xlab = "Z", main = "100 Z~N(0, 1)")
lines(x.hist, dnorm(x.hist), col = 2, lty = 1)
par(mfrow = c(1, 1))

sample_quantile_5 = sort(r.vec[n.vec == 5])
sample_quantile_10 = sort(r.vec[n.vec == 10])
sample_quantile_50 = sort(r.vec[n.vec == 50])
sample_quantile_100 = sort(r.vec[n.vec == 100])
sample_quantile_500 = sort(r.vec[n.vec == 500])
sample_cdf = (1:num)/num
plot(sample_quantile_5, sample_cdf, type = "s",
     xlab = "", ylab = "",
     main = "Cumulative distribution function n = 500")
lines(sample_quantile_10, sample_cdf, col = 3, lty = 4)
lines(sample_quantile_50, sample_cdf, col = 4, lty = 4)
lines(sample_quantile_100, sample_cdf, col = 5, lty = 4)
lines(sample_quantile_500, sample_cdf, col = 6, lty = 4)
lines(x.hist, pnorm(x.hist), col = 2, lty = 2)
legend("topleft", lty = c(1, 2), col = c(1, 2), 
       legend = c("Estimation", "True CDF"))

